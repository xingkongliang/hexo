<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker 安装与使用]]></title>
    <url>%2F2018%2F10%2F10%2Fdocker%2F</url>
    <content type="text"><![CDATA[Docker 安装与使用@(工具学习记录)[Docker] 1. Docker安装参考官网教程 卸载旧的版本1$ sudo apt-get remove docker docker-engine docker.iSET UP THE REPOSITORY SET UP THE REPOSITORY 1$ sudo apt-get update 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 123456$ sudo apt-key fingerprint 0EBFCD88pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) &lt;docker@docker.com&gt;sub 4096R/F273FCD8 2017-02-22 x86_64/amd641234$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 安装 DOCKER CE更新包的索引1$ sudo apt-get update 安装最新版本的Docker CE1$ sudo apt-get install docker-ce 安装特定版本Docker CE123$ apt-cache madison docker-cedocker-ce | 18.03.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages 验证是否安装正确1$ sudo docker run hello-world 2.nvidia-docker 安装参考nvidia-dockernstalling version 2.0 Debian-based distributions123456curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \ sudo apt-key add -distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \ sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get update 12sudo apt-get install nvidia-docker2sudo pkill -SIGHUP dockerd 3. detectron 配置参考detectron install12cd $DETECTRON/dockerdocker build -t detectron:c2-cuda9-cudnn7 . 运行这个镜像1nvidia-docker run --rm -it detectron:c2-cuda9-cudnn7 python detectron/tests/test_batch_permutation_op.py 4. docker 基本命令对容器生命周期管理rundocker run ：创建一个新的容器并运行一个命令12345678910111213141516使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。docker run --name mynginx -d nginx:latest使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。docker run -P -d nginx:latest使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。docker run -p 80:80 -v /data:/data -d nginx:latest绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。$ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。runoob@runoob:~$ docker run -it nginx:latest /bin/bashroot@b8573233d675:/# start/stop/restartkillrmdocker rm ：删除一个或多少容器 语法1docker rm [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷docker rm ：删除一个或多少容器 语法docker rm [OPTIONS] CONTAINER [CONTAINER…]OPTIONS说明： -f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷123456789强制删除容器db01、db02docker rm -f db01 db02移除容器nginx01对容器db01的连接，连接名dbdocker rm -l db 删除容器nginx01,并删除容器挂载的数据卷docker rm -v nginx01 pause/unpausecreateexecdocker exec ：在运行的容器中执行命令12345678在容器mynginx中以交互模式执行容器内/root/runoob.sh脚本runoob@runoob:~$ docker exec -it mynginx /bin/sh /root/runoob.shhttp://www.runoob.com/在容器mynginx中开启一个交互模式的终端runoob@runoob:~$ docker exec -i -t mynginx /bin/bashroot@b1a0703e41e7:/# commit 命令docker commit :从容器创建一个新的镜像。 -a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。12345runoob@runoob:~$ docker commit -a &quot;runoob.com&quot; -m &quot;my apache&quot; a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057runoob@runoob:~$ docker images mymysql:v1REPOSITORY TAG IMAGE ID CREATED SIZEmymysql v1 37af1236adef 15 seconds ago 329 MB 容器与本地之间拷贝文件12345将主机./RS-MapReduce目录拷贝到容器30026605dcfe的/home/cloudera目录下。docker cp RS-MapReduce 30026605dcfe:/home/cloudera将容器30026605dcfe的/home/cloudera/RS-MapReduce目录拷贝到主机的/tmp目录中。docker cp 30026605dcfe:/home/cloudera/RS-MapReduce /tmp/ 5. 学习资源 runoob docker 只要一小时，零基础入门Docker]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conda使用指南]]></title>
    <url>%2F2018%2F09%2F09%2FConda-Tutorials%2F</url>
    <content type="text"><![CDATA[Python渐渐成为最流行的编程语言之一，在数据分析、机器学习和深度学习等方向Python语言更是主流。Python的版本比较多，并且它的库也非常广泛，同时库和库之间存在很多依赖关系，所以在库的安装和版本的管理上很麻烦。Conda是一个管理版本和Python环境的工具，它使用起来非常容易。 首先你需要安装Anconda软件，点击链接download。选择对应的系统和版本类型。 Conda的环境管理创建环境12# 创建一个名为python34的环境，指定Python版本是3.5（不用管是3.5.x，conda会为我们自动寻找3.５.x中的最新版本）conda create --name py35 python=3.5 激活环境123456789# 安装好后，使用activate激活某个环境activate py35 # for Windowssource activate py35 # for Linux &amp; Mac(py35) user@user-XPS-8920:~$ # 激活后，会发现terminal输入的地方多了py35的字样，实际上，此时系统做的事情就是把默认2.7环境从PATH中去除，再把3.4对应的命令加入PATH (py35) user@user-XPS-8920:~$ python --versionPython 3.5.5 :: Anaconda, Inc.# 可以得到`Python 3.5.5 :: Anaconda, Inc.`，即系统已经切换到了3.５的环境 返回主环境123# 如果想返回默认的python 2.7环境，运行deactivate py35 # for Windowssource deactivate py35 # for Linux &amp; Mac 删除环境12# 删除一个已有的环境conda remove --name py35 --all 查看系统中的所有环境用户安装的不同Python环境会放在~/anaconda/envs目录下。查看当前系统中已经安装了哪些环境，使用conda info -e。 1234567user@user-XPS-8920:~$ conda info -e# conda environments:#base * /home/user/anaconda2caffe /home/user/anaconda2/envs/caffepy35 /home/user/anaconda2/envs/py35tf /home/user/anaconda2/envs/tf Conda的包管理安装库为当前环境安装库123# numpyconda install numpy# conda会从从远程搜索numpy的相关信息和依赖项目 ### 查看已经安装的库 123# 查看已经安装的packagesconda list# 最新版的conda是从site-packages文件夹中搜索已经安装的包，可以显示出通过各种方式安装的包 查看某个环境的已安装包12# 查看某个指定环境的已安装包conda list -n py35 搜索package的信息12# 查找package信息conda search numpy 12345678Loading channels: done# Name Version Build Channel numpy 1.5.1 py26_1 pkgs/free ...numpy 1.15.1 py37hec00662_0 anaconda/pkgs/main numpy 1.15.1 py37hec00662_0 pkgs/main 安装package到指定的环境1234# 安装packageconda install -n py35 numpy# 如果不用-n指定环境名称，则被安装在当前活跃环境# 也可以通过-c指定通过某个channel安装 更新package12# 更新packageconda update -n py35 numpy 删除package12# 删除packageconda remove -n py35 numpy 更新conda12# 更新conda，保持conda最新conda update conda 更新anaconda12# 更新anacondaconda update anaconda 更新Python123# 更新pythonconda update python# 假设当前环境是python 3.5, conda会将python升级为3.5.x系列的当前最新版本 设置国内镜像因为Anaconda.org的服务器在国外，所有有些库下载缓慢，可以使用清华Anaconda镜像源。 网站地址: 清华大学开源软件镜像站 Anaconda 镜像Anaconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 下载。 TUNA还提供了Anaconda仓库的镜像，运行以下命令：123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 即可添加 Anaconda Python 免费仓库。 运行 conda install numpy 测试一下吧。 Miniconda 镜像Miniconda 是一个 Anaconda 的轻量级替代，默认只包含了 python 和 conda，但是可以通过 pip 和 conda 来安装所需要的包。 Miniconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ 下载。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[MLIA] Logistic Regression]]></title>
    <url>%2F2018%2F09%2F05%2FMLIA-Logistic-Regression%2F</url>
    <content type="text"><![CDATA[Logistic Regression本代码来自Machine Learning in Action。 想要了解更多的朋友可以参考此书。 Sigmoid函数$$\sigma(z) = \frac{1}{(1+e^{-z})}$$ 12345678910import numpy as npimport matplotlib.pyplot as pltdef sigmoid(inX): return 1.0/(1+np.exp(-inX))z = np.linspace(-5, 5, 100)y = sigmoid(z)plt.plot(z, y)plt.show() 1234z = np.linspace(-60, 60, 100)y = sigmoid(z)plt.plot(z, y)plt.show() Sigmoid函数类似一个单位阶跃函数。当x＝0时，Sigmoid函数值为0.5；随着x增大，Sigmoid函数值将逼近于1；随着x减小，Sigmoid函数将逼近于0。利用这个性质可以对它的输入做一个二分类。 为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把它的所有的结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间是数值。当大于0.5的时候，将数据分类为1；当小于0.5的时候，将数据分类为0。 Sigmoid函数的输入记为z: $$z=w_0x_0 + w_1x_1 + w_2x_2 + \cdot \cdot \cdot + w_n x_n$$ Sigmoid函数的导数Sigmoid导数具体推导过程如下： $$\begin{align}f^{\prime}(z) &amp;= (\frac{1}{1+e^{-z}})^{\prime}\\&amp;=\frac{e^{-z}}{(1+e^{-z})^2}\\&amp;=\frac{1+e^{-z}-1}{(1+e^{-z})^2}\\&amp;=\frac{1}{(1+e^{-z})}(1-\frac{1}{(1+e^{-z})})\\&amp;=f(z)(1-f(z))\end{align}$$ 梯度上升法梯度上升法：顾名思义就是利用梯度方向，寻找到某函数的最大值。 梯度上升算法迭代公式：$$w:=w+\alpha \nabla_w f(w)$$ 梯度下降法：和梯度上升想法，利用梯度方法，寻找某个函数的最小值。梯度下降算法迭代公式：$$w:=w-\alpha \nabla_w f(w)$$ 梯度上升算法每次更新之后，都会重新估计移动的方法，即梯度。 Logistic 回归梯度上升优化法加载数据12345678def loadDataSet(): dataMat = []; labelMat = [] fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat,labelMat 12345dataArray, labelMat = loadDataSet()print("Total: ", len(dataArray))print("The first sample: ", dataArray[0])print("The second sample: ", dataArray[1])print("Label: ", labelMat) (&apos;Total: &apos;, 100) (&apos;The first sample: &apos;, [1.0, -0.017612, 14.053064]) (&apos;The second sample: &apos;, [1.0, -1.395634, 4.662541]) (&apos;Label: &apos;, [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0]) 数据集梯度上升123456789101112131415def sigmoid(inX): return 1.0/(1+np.exp(-inX))def gradAscent(dataMatIn, classLabels): dataMatrix = np.mat(dataMatIn) #convert to NumPy matrix labelMat = np.mat(classLabels).transpose() #convert to NumPy matrix m,n = np.shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = np.ones((n,1)) for k in range(maxCycles): #heavy on matrix operations h = sigmoid(dataMatrix*weights) #matrix mult error = (labelMat - h) #vector subtraction weights = weights + alpha * dataMatrix.transpose()* error #matrix mult return weights 1gradAscent(dataArray, labelMat) matrix([[ 4.12414349], [ 0.48007329], [-0.6168482 ]]) 绘制数据和决策边界123456789101112131415161718192021def plotBestFit(weights): import matplotlib.pyplot as plt dataMat,labelMat=loadDataSet() dataArr = np.array(dataMat) n = np.shape(dataArr)[0] xcord1 = []; ycord1 = [] xcord2 = []; ycord2 = [] for i in range(n): if int(labelMat[i])== 1: xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') x = np.arange(-3.0, 3.0, 0.1) y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x, y) plt.xlabel('X1'); plt.ylabel('X2'); plt.show() 12weights = gradAscent(dataArray, labelMat)plotBestFit(weights.getA()) １个epoch的随机梯度上升梯度上升算法在每次更新系数的时候都需要便利整个数据集，如果数据集的样本比较大，该方法的复杂度和计算代价就很高。有一种改进的方法叫做随机梯度上升方法。该方法的思想是选取一个样本，计算该样本的梯度，更新系数，再选取下一个样本。 123456789def stocGradAscent0(dataMatrix, classLabels): m,n = np.shape(dataMatrix) alpha = 0.01 weights = np.ones(n) #initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights 12weights = stocGradAscent0(np.array(dataArray), labelMat)plotBestFit(weights) 上图之后遍历了一次数据集，这样的模型还处于欠拟合状态。需要多次遍历数据集才能优化好模型，接下来我们会运行200次迭代。 200个epoch的随机梯度上升1234567891011121314def stocGradAscent0(dataMatrix, classLabels): X0, X1, X2 = [], [], [] m,n = np.shape(dataMatrix) alpha = 0.01 weights = np.ones(n) #initialize to all ones for j in range(200): for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] X0.append(weights[0]) X1.append(weights[1]) X2.append(weights[2]) return weights, X0, X1, X2 12weights, X0, X1, X2 = stocGradAscent0(np.array(dataArray), labelMat)plotBestFit(weights) 可视化权重(weights)的变化12345fig, ax = plt.subplots(3, 1, figsize=(10, 5))ax[0].plot(np.arange(len(X0)), np.array(X0))ax[1].plot(np.arange(len(X1)), np.array(X1))ax[2].plot(np.arange(len(X2)), np.array(X2))plt.show() 从上图可以看出，算法正在逐渐收敛。由于数据集并不是线性可分的，所以存在一些不能正确分类的样本点，每次更新权重引起了周期的变化。 更新过后的随机梯度上升算法 学习率alpha会在每次迭代之后调整。 采用随机选取样本的更新策略，减少周期性的波动。 1234567891011121314151617def stocGradAscent1(dataMatrix, classLabels, numIter=150): X0, X1, X2 = [], [], [] m,n = np.shape(dataMatrix) weights = np.ones(n) #initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(np.random.uniform(0,len(dataIndex)))#go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] X0.append(weights[0]) X1.append(weights[1]) X2.append(weights[2]) del(dataIndex[randIndex]) return weights, X0, X1, X2 12weights, X0, X1, X2 = stocGradAscent1(np.array(dataArray), labelMat)plotBestFit(weights) 可视化权重(weights)的变化12345fig, ax = plt.subplots(3, 1, figsize=(10, 5))ax[0].plot(np.arange(len(X0)), np.array(X0))ax[1].plot(np.arange(len(X1)), np.array(X1))ax[2].plot(np.arange(len(X2)), np.array(X2))plt.show() 示例：从疝气病症预测病马的死亡率12345678910111213141516171819202122232425262728293031323334def classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt', 'r'); frTest = open('horseColicTest.txt', 'r') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights, X0, X1, X2 = stocGradAscent1(np.array(trainingSet), trainingLabels, 1000) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(np.array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print "the error rate of this test is: %f" % errorRate return errorRatedef multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print "after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests)) 1multiTest() /home/tianliang/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp the error rate of this test is: 0.328358 the error rate of this test is: 0.432836 the error rate of this test is: 0.388060 the error rate of this test is: 0.373134 the error rate of this test is: 0.373134 the error rate of this test is: 0.447761 the error rate of this test is: 0.343284 the error rate of this test is: 0.313433 the error rate of this test is: 0.328358 the error rate of this test is: 0.462687 after 10 iterations the average error rate is: 0.379104]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CornerNet: Detection Objects as Paired Keypoints]]></title>
    <url>%2F2018%2F09%2F02%2FCornerNet-Detection-Objects-as-Paired-Keypoints%2F</url>
    <content type="text"><![CDATA[前言CornerNet: Detection Objects as Paired Keypoints 这篇论文发表在ECCV2018，本人感觉非常有意思，所以和大家分享一下。 Arxiv: https://arxiv.org/abs/1808.01244Github: https://github.com/umich-vl/ 介绍传统的目标检测都是给出紧致的候选框，本论文独具匠心，通过一对关键点（目标的左上角和右下角）来检测一个目标框。通过检测关键点的这种方式，可以消除利用先验知识设计anchor boxes这个需求。作者提出角点池化（corner pooling），角点池化可以帮助网络更好的定位角点。最终实验表明，CornerNet在MS COCO数据集上实现了42.1%的AP，优于所有现存的单级(one-stage)检测器。]]></content>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测（Pedestrian Detection）论文整理]]></title>
    <url>%2F2018%2F08%2F17%2FPedestrian-Detection-Sources%2F</url>
    <content type="text"><![CDATA[相关科研工作者 张姗姗 scholar 张姗姗 homepage 欧阳万里 scholar 欧阳万里 homepage 论文[TIP-2018] Too Far to See? Not Really: Pedestrian Detection with Scale-Aware Localization Policy paper: project website: slides: github: [Transactions on Multimedia-201８] Scale-Aware Fast R-CNN for Pedestrian Detection paper: https://ieeexplore.ieee.org/abstract/document/8060595/ project website: slides: github: [ECCV-2018] Bi-box Regression for Pedestrian Detection and Occlusion Estimation arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf slides: github: [ECCV-2018] Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf project website: slides: github: [ECCV-2018] Graininess-Aware Deep Feature Learning for Pedestrian Detection arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/Chunze_Lin_Graininess-Aware_Deep_Feature_ECCV_2018_paper.pdf project website: slides: github: [ECCV-2018] Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd arxiv:http://arxiv.org/abs/1807.08407 project website: slides: github: [ECCV-2018] Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation arxiv:https://arxiv.org/abs/1807.01438 project website: slides: github: [CVPR-2018] Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors arxiv: paper: http://vision.snu.ac.kr/projects/partgridnet/data/noh_2018.pdf project website: http://vision.snu.ac.kr/projects/partgridnet/ slides: github: [CVPR-2018] Occluded Pedestrian Detection Through Guided Attention in CNNs arxiv: paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf project website: slides: github: [CVPR-2018] Repulsion Loss: Detecting Pedestrians in a Crowd arxiv:http://arxiv.org/abs/1711.07752 project website: slides: github: blog: https://zhuanlan.zhihu.com/p/41288115 [TPAMI-2017] Jointly Learning Deep Features, Deformable Parts, Occlusion and Classification for Pedestrian Detection paper: https://ieeexplore.ieee.org/abstract/document/8008790/ project website: slides: github caffe: [BMVC-2017] PCN: Part and Context Information for Pedestrian Detection with CNNs arxiv: https://arxiv.org/abs/1804.044838 project website: slides: github caffe: [CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection arxiv: http://arxiv.org/abs/1702.05693 project website: slides: github caffe: [CVPR-2017] Learning Cross-Modal Deep Representations for Robust Pedestrian Detection arxiv: https://arxiv.org/abs/1704.02431 project website: slides: github caffe: [CVPR-2017] What Can Help Pedestrian Detection? arxiv: https://arxiv.org/abs/1704.02431 project website: slides: github caffe: [TPAMI-2017] Towards Reaching Human Performance in Pedestrian Detection paper: http://ieeexplore.ieee.org/document/7917260/ arxiv: project website: slides: github caffe: [ICCV-2017] Multi-label Learning of Part Detectors for Heavily Occluded Pedestrian Detection paper: http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Multi-Label_Learning_of_ICCV_2017_paper.pdf arxiv: project website: slides: [ICCV-2017]Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation arxiv: https://arxiv.org/abs/1706.08564 project website: http://cvlab.cse.msu.edu/project-pedestrian-detection.html slides: github caffe: https://github.com/garrickbrazil/SDS-RCNN [CVPR-2016] Semantic Channels for Fast Pedestrian Detection paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Costea_Semantic_Channels_for_CVPR_2016_paper.pdf project website: slides: github caffe: [CVPR-2016] How Far areWe from Solving Pedestrian Detection? paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S06-29.pdf project website: slides: github caffe: [ICCV-2015] Deep Learning Strong Parts for Pedestrian Detection paper: https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tian_Deep_Learning_Strong_ICCV_2015_paper.htmler.html project website: slides: github caffe: [CVPR-2013] Joint Deep Learning for Pedestrian Detection Wanli paper: https://www.cv-foundation.org/openaccess/content_iccv_2013/html/Ouyang_Joint_Deep_Learning_2013_ICCV_paper.html project website: slides: github caffe: [CVPR-2012] A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling paper: http://mmlab.ie.cuhk.edu.hk/pdf/ouyangWcvpr2012.pdf paper: https://ieeexplore.ieee.org/abstract/document/6248062/ project website: slides: github caffe: [CVPR-2010] Multi-Cue Pedestrian Classification With Partial Occlusion Handling paper: https://ieeexplore.ieee.org/abstract/document/5540111/ project website: slides: github caffe: 行人检测数据集CityPersons CityPersons数据集是在Cityscapes数据集基础上建立的，使用了Cityscapes数据集的数据，对一些类别进行了精确的标注。该数据集是在[CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection这篇论文中提出的，更多细节可以通过阅读该论文了解。 上图中左侧是行人标注，右侧是原始的CityScapes数据集。 标注和评估文件 数据集下载 文件格式 123456789101112131415#评测文件$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/coco.py$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/eval_demo.py$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/eval_MR_multisetup.py#注释文件$/Cityscapes/shanshanzhang-citypersons/annotations$/Cityscapes/shanshanzhang-citypersons/annotations/anno_train.mat$/Cityscapes/shanshanzhang-citypersons/annotations/anno_val.mat$/Cityscapes/shanshanzhang-citypersons/annotations/README.txt#图片数据$/Cityscapes/leftImg8bit/train/*$/Cityscapes/leftImg8bit/val/*$/Cityscapes/leftImg8bit/test/* 注释文件格式123456789101112131415161718192021222324CityPersons annotations(1) data structure: one image per cell in each cell, there are three fields: city_name; im_name; bbs (bounding box annotations)(2) bounding box annotation format: one object instance per row: [class_label, x1,y1,w,h, instance_id, x1_vis, y1_vis, w_vis, h_vis](3) class label definition: class_label =0: ignore regions (fake humans, e.g. people on posters, reflections etc.) class_label =1: pedestrians class_label =2: riders class_label =3: sitting persons class_label =4: other persons with unusual postures class_label =5: group of people(4) boxes: visible boxes [x1_vis, y1_vis, w_vis, h_vis] are automatically generated from segmentation masks; (x1,y1) is the upper left corner. if class_label==1 or 2 [x1,y1,w,h] is a well-aligned bounding box to the full body ; else [x1,y1,w,h] = [x1_vis, y1_vis, w_vis, h_vis]; Caltech Caltech官网更所细节请阅读这篇论文，[TAPAMI-2012] Pedestrian Detection: An Evaluation of the State of the Art KITTI KITTI官网]]></content>
      <tags>
        <tag>Pedestrian Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras Tutorial]]></title>
    <url>%2F2018%2F08%2F14%2FKeras-Tutorial%2F</url>
    <content type="text"><![CDATA[Github地址：here Keras-Tutorials 版本：0.0.1 作者：张天亮 邮箱：zhangtianliang13@mails.ucas.ac.cn Github 加载 .ipynb 的速度较慢，建议在 Nbviewer 中查看该项目。 简介大部分内容来自keras项目中的examples 目录 01.多层感知机实现 02.模型的保存 03.模型的加载 04.绘制精度和损失曲线 05.卷积神经网络实现 06.CIFAR10_cnn 07.mnist_lstm 08.VGG16调用 09.卷积滤波器可视化 10.variational_autoencoder 11.锁定层fine-tuning网络 12.使用sklearn wrapper进行的参数搜索 13.Keras和Tensorflow联合使用 14.Finetune InceptionV3样例 15.自编码器 16.卷积自编码器 更多Keras使用方法请查看手册 中文手册 英文手册 github]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Nets]]></title>
    <url>%2F2018%2F08%2F14%2FGenerative-Adversarial-Nets%2F</url>
    <content type="text"><![CDATA[DCGANs in TensorFlowcarpedm20/DCGAN-tensorflow我们定义网络结构： 123456789101112131415161718192021222324252627282930313233343536def generator(self, z): self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, 'g_h0_lin', with_w=True) self.h0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = conv2d_transpose(h0, [self.batch_size, 8, 8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = conv2d_transpose(h1, [self.batch_size, 16, 16, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = conv2d_transpose(h2, [self.batch_size, 32, 32, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = conv2d_transpose(h3, [self.batch_size, 64, 64, 3], name='g_h4', with_w=True) return tf.nn.tanh(h4)def discriminator(self, image, reuse=False): if reuse: tf.get_variable_scope().reuse_variables() h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) h4 = linear(tf.reshape(h3, [-1, 8192]), 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 当我们初始化这个类时，我们将使用这些函数来创建模型。 我们需要两个版本的鉴别器共享（或重用）参数。 一个用于来自数据分布的图像的minibatch，另一个用于来自发生器的图像的minibatch。 123self.G = self.generator(self.z)self.D, self.D_logits = self.discriminator(self.images)self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) 接下来我们定义损失函数。我们在D的预测值和我们理想的判别器输出值之间使用交叉熵，而没有只用求和，因为这样的效果更好。判别器希望对“真实”数据的预测全部是1，并且来自生成器的“假”数据的预测全部是零。生成器希望判别器对所有假样本的预测都是1。 1234567891011self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))self.d_loss = self.d_loss_real + self.d_loss_fakeself.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))) 收集每个模型的变量，以便可以单独进行训练。 1234t_vars = tf.trainable_variables()self.d_vars = [var for var in t_vars if 'd_' in var.name]self.g_vars = [var for var in t_vars if 'g_' in var.name] 现在我们准备好优化参数，我们将使用ADAM，这是一种在现代深度学习中常见的自适应非凸优化方法。ADAM通常与SGD竞争，并且（通常）不需要手动调节学习速率，动量和其他超参数。 1234d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.d_loss, var_list=self.d_vars)g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.g_loss, var_list=self.g_vars) 我们已经准备好了解我们的数据。在每个epoch中，我们在每个minibatch中采样一些图像，并且运行优化器更新网络。有趣的是，如果G仅更新一次，判别器的损失则不会为零。另外，我认为d_loss_fake和d_loss_real在最后的额外的调用回到是一点点不必要的计算，并且是冗余的，因为这些值是作为d_optim和g_optim的一部分计算的。作为TensorFlow中的练习，您可以尝试优化此部分并将RP发送到原始库。 1234567891011121314151617181920212223for epoch in xrange(config.epoch): ... for idx in xrange(0, batch_idxs): batch_images = ... batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \ .astype(np.float32) # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict=&#123; self.images: batch_images, self.z: batch_z &#125;) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z &#125;) # Run g_optim twice to make sure that d_loss does not go to zero # (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z &#125;) errD_fake = self.d_loss_fake.eval(&#123;self.z: batch_z&#125;) errD_real = self.d_loss_real.eval(&#123;self.images: batch_images&#125;) errG = self.g_loss.eval(&#123;self.z: batch_z&#125;) Generative Adversarial Networks代码整理 InfoGAN-TensorFlow:InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets iGAN-Theano:Generative Visual Manipulation on the Natural Image Manifold SeqGAN-TensorFlow:SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient DCGAN-Tensorflow:Deep Convolutional Generative Adversarial Networks dcgan_code-Theano:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks improved-gan-Theano:Improved Techniques for Training GANs chainer-DCGAN:Chainer implementation of Deep Convolutional Generative Adversarial Network keras-dcgan]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to use hexo?]]></title>
    <url>%2F2018%2F08%2F14%2FHow-to-use-hexo%2F</url>
    <content type="text"><![CDATA[Hexo基本指令init1$ hexo init [folder] 新建一个网站，如果没有设置｀folder｀，Hexo默认在目前的文件夹建立网站。 new1$ hexo new [layout] &lt;title&gt; 新建一片文章。如果没有设置layout的话，默认使用_config.yml中的default_layout参数代替。如果标题包含空格的话，请使用引号括起来。 generate12$ hexo generate$ hexo g # 简写 publish1$ hexo publish [layout] &lt;filename&gt; 发表草稿。 server1$ hexo server 启动服务器。默认情况下，访问网址为：http://localhost:4000/。 选项 描述 -p,--port 重设端口 -s, --static 只使用静态文本 -l,--log 启动日记记录，使用覆盖记录格式 deploy12$ hexo deploy$ hexo d # 简写 render1$ hexo render &lt;file1&gt; [file2] ... 渲染文件。 clean1$ hexo clean 清楚缓存文件(db.json)好已经生成的静态文件(public)。 在某些情况(尤其是更换主题后)，如果发现您对站点的更改没有生效，可以使用此命令。 list1$ hexo list &lt;type&gt; 列出网站资料。 version1$ hexo version 显示Hexo版本。 显示草稿1$ hexo --draft 显示source/_drafts文件夹中的草稿文件。 自定义CWD1$ hexo --cwd /path/to/cwd 自定义当前工作目录(Current working dirctory)的路径。 在主页截断 方法1:在文中插入以下代码 1&lt;!--more--&gt; 方法2:在文章中的front-matter中添加description， 1234567---title: date: 2018-08-14 15:21:22categories: tags: description: 描述。。--- 分类和标签只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games 定义一段代码。12345import osimport numpy as npfor i in range(10): print('now is ', i) 显示一幅图像。1&#123;% asset_img 003.jpg This is an example image %&#125; Hexo 资源hexo官网 theme-next使用说明 使用hexo，如果换了电脑怎么更新博客？ 其他参考博客：我的个人博客之旅：从jekyll到hexoHEXO+NEXT主题个性化配置 hexo的next主题个性化配置教程 基于Hexo+Node.js+github+coding搭建个人博客——进阶篇(从入门到入土)]]></content>
  </entry>
</search>
