<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Pandas Tutorial]]></title>
    <url>%2F2019%2F06%2F23%2FPandas-Tutorial%2F</url>
    <content type="text"><![CDATA[10 Minutes to pandas本文原网址 导入所需要的包。123In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 目标创建通过传递一个列表创建 Series，让pandas创建一个默认的整型索引：1234567891011In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]:0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 通过传递一个Numpy数组创建一个DataFrame数据，用时间和有标签的列作为索引：12345678910111213141516171819In [6]: dates = pd.date_range(&apos;20130101&apos;, periods=6)In [7]: datesOut[7]:DatetimeIndex([&apos;2013-01-01&apos;, &apos;2013-01-02&apos;, &apos;2013-01-03&apos;, &apos;2013-01-04&apos;, &apos;2013-01-05&apos;, &apos;2013-01-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)In [8]: df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&apos;ABCD&apos;))In [9]: dfOut[9]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 通过传递一个序列对象的字典创建DataFrame。123456789101112131415In [10]: df2 = pd.DataFrame(&#123; &apos;A&apos; : 1., ....: &apos;B&apos; : pd.Timestamp(&apos;20130102&apos;), ....: &apos;C&apos; : pd.Series(1,index=list(range(4)),dtype=&apos;float32&apos;), ....: &apos;D&apos; : np.array([3] * 4,dtype=&apos;int32&apos;), ....: &apos;E&apos; : pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), ....: &apos;F&apos; : &apos;foo&apos; &#125;) ....:In [11]: df2Out[11]: A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train foo 得到的DataFrame的列有不同的类型：123456789In [12]: df2.dtypesOut[12]:A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 浏览数据可以看基本章节。 这里我们查看一下frame的前几行和后几行：123456789101112131415In [14]: df.head()Out[14]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401In [15]: df.tail(3)Out[15]: A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 显示索引和列，并且显示隐含的NumPy数据：1234567891011121314151617In [16]: df.indexOut[16]:DatetimeIndex([&apos;2013-01-01&apos;, &apos;2013-01-02&apos;, &apos;2013-01-03&apos;, &apos;2013-01-04&apos;, &apos;2013-01-05&apos;, &apos;2013-01-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)In [17]: df.columnsOut[17]: Index([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;D&apos;], dtype=&apos;object&apos;)In [18]: df.valuesOut[18]:array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]]) describe()显示一个快速的你的数据的统计信息：1234567891011In [19]: df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 转置你的数据：1234567In [20]: df.TOut[20]: 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988 通过一个维度进行排序：123456789In [21]: df.sort_index(axis=1, ascending=False)Out[21]: D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690 通过数值排序：123456789In [22]: df.sort_values(by=&apos;B&apos;)Out[22]: A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 选择得到数据选择一个列，这会产生一个Series， 等同于df.A：123456789In [23]: df[&apos;A&apos;]Out[23]:2013-01-01 0.4691122013-01-02 1.2121122013-01-03 -0.8618492013-01-04 0.7215552013-01-05 -0.4249722013-01-06 -0.673690Freq: D, Name: A, dtype: float64 通过[]进行选择，这可以切开行：12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df[&apos;20130102&apos;:&apos;20130104&apos;]Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860 通过标签选择更多请看here。 使用标签获得一个截面：1234567In [26]: df.loc[dates[0]]Out[26]:A 0.469112B -0.282863C -1.509059D -1.135632Name: 2013-01-01 00:00:00, dtype: float64 通过标签选择多个轴线：123456789In [27]: df.loc[:,[&apos;A&apos;,&apos;B&apos;]]Out[27]: A B2013-01-01 0.469112 -0.2828632013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670202013-01-06 -0.673690 0.113648 显示一个标签切片，并且也包括结束点：123456In [28]: df.loc[&apos;20130102&apos;:&apos;20130104&apos;,[&apos;A&apos;,&apos;B&apos;]]Out[28]: A B2013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.706771 可视化基础绘画: plot123456In [2]: ts = pd.Series(np.random.randn(1000), index=pd.date_range(&apos;1/1/2000&apos;, periods=1000))In [3]: ts = ts.cumsum()In [4]: ts.plot()Out[4]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c2ead5a20&gt; 1234In [15]: plt.figure();In [16]: df.iloc[5].plot.bar(); plt.axhline(0, color=&apos;k&apos;)Out[16]: &lt;matplotlib.lines.Line2D at 0x1c318b4f60&gt; 123In [17]: df2 = pd.DataFrame(np.random.rand(10, 4), columns=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])In [18]: df2.plot.bar(); 直方图（Histograms）12345678In [21]: df4 = pd.DataFrame(&#123;&apos;a&apos;: np.random.randn(1000) + 1, &apos;b&apos;: np.random.randn(1000), ....: &apos;c&apos;: np.random.randn(1000) - 1&#125;, columns=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]) ....:In [22]: plt.figure();In [23]: df4.plot.hist(alpha=0.5)Out[23]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c2f3fb2e8&gt; 1234In [24]: plt.figure();In [25]: df4.plot.hist(stacked=True, bins=20)Out[25]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1233ad2b0&gt; 1234In [28]: plt.figure();In [29]: df[&apos;A&apos;].diff().hist()Out[29]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1c333967f0&gt; 123456789In [30]: plt.figure()Out[30]: &lt;Figure size 640x480 with 0 Axes&gt;In [31]: df.diff().hist(color=&apos;k&apos;, alpha=0.5, bins=50)Out[31]:array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2b9669e8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c3184a0b8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2e766668&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c319e1240&gt;]], dtype=object) 12345678In [32]: data = pd.Series(np.random.randn(1000))In [33]: data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4))Out[33]:array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2f245898&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2fd204a8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2f326240&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1c2e751b00&gt;]], dtype=object)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变量——看见社会小趋势]]></title>
    <url>%2F2019%2F06%2F20%2FBook-BianLiang%2F</url>
    <content type="text"><![CDATA[作者简介何帆，男，现任北京大学汇丰商学院经济学教授，兼熵一资本首席经济学家。曾任中国社会科学院世界经济与政治研究所副所长，在政策研究领域研究已经超过20年 [3] ，发表学术论文100多篇，出版专著10余部，如《变量》《何帆大局观》等 。现被厦门大学EMBA管理学院特聘为EMBA讲师，同陆磊教授一同讲授EMBA课程-《宏观经济理论与实践》。同时，何帆是得到App《何帆大局观》《何帆的读书俱乐部》《何帆报告》课程主理人。 作者简介来自百度百科 摘抄第一章 这样观察一棵树2018年是一个新的开端。生活在2018年的人感受到的是中国经济遇到的各种冲击：中美贸易战、经济增长回落、股市下跌。他们会感到焦虑和担忧。旧路标已经消失，新秩序尚未出现。未来30年出现的一系列变化将挑战我们的认知，但历史从来都是一位“魔术师”，未来会出现意想不到的变化。在这一章，我会讲述如何像细致地观察一棵树一样观察历史，怎样从每年长出的“嫩芽”去判断中国文明这棵大树的生命力。我还会告诉你两个重要的概念：慢变量和小趋势。感知历史，就要会从慢变量中寻找小趋势。 第二章 在无人地带寻找无人机2018年，关于技术发展路径的讨论引起全民关注。中国到底是应该集中全力补上“核心技术”，还是应该扬己所长发展“应用技术”呢？我将带你回顾美国在工业革命时期的经验，并试图发现中国在信息化时代的最佳战略。我找到的第二个变量是：技术赋能。在创新阶段，寻找新技术的应用场景更重要，在边缘地带更容易找到新技术的应用场景，技术必须与市场需求匹配。我们会到新疆去看无人机，而你很可能会在酒店里邂逅机器人。中国革命的成功靠的是“群众路线”，中国经济的崛起也要走“群众路线”。 ## 第三章 老兵不死 2018年，谁是新兴产业，谁是传统产业？哪个更胜一筹？在过去几年，互联网大军就好像当年来自中亚大草原的游牧民族，兵强马壮，来去如风。在互联网大军的攻势下，传统产业的护城河形同虚设。到了2018年，这股“为跨不破”，精于“降维打击”的大军，却在一座城堡前久攻不下。这就是工业化的代表————已经有上百年历史的汽车行业。2018年，我发现的第三个变量是：老兵不死。我要带你到传统制造业的腹地，看看他们是如何抵御互联网行业的迅猛攻势。在这里，你会看到，传统行业的老兵早已经悄悄穿上了新的军装，而新兴的产业正在积极地想传统产业学习。新兴产业和传统产业的边界，也许并没有你想象的那般泾渭分明。 ## 第四章 在菜市场遇见城市设计师 2018年，人们最关心的是房价是否会出现拐点，但从长时间来看更值得关注的是城市化的拐点。自上而下的城市化已不可持续。我观察到的第四个变量是：自下而上的力量浮出水面。城市化的进程不会停止，未来会有更多的城市圈，但这些都市圈是放大了的城市，还是一种新的城市物种呢？未来的城市不一定都能扩张，假如城市不得不“收缩”，该怎样才能像瘦身一样，瘦了更健康？未来的城市将深受互联网影响，城市空间布局会跟过去有很大的不同。“位置、位置、位置”的传统房地产“金律”很可能不再适用。我们会看到，城市会爆发一场“颜值革命”。这场“颜值革命”来自哪里呢？归根到底，它来自人民群众自己创造美好生活的能量。 第五章 阿那亚和范家小学2018年，我们听到了很多负面的社会新闻：米脂杀人、衡阳装车、高铁霸座……这个社会变得越来越糟糕了吗？其中这是一种误解。虽然从表明上看，有些人只关心自我私利，但大家对集体生活的向往并没有泯灭。中国人已经意识到，只有重建集体生活，才能更好地发现自我。我看到的第五个变量就是：重建社群。有哪些地方的人们正在“凝结”起来，形成新的社群？这些新的社群只是孤岛，还是将成为群岛？培养孩子也需要一个社群。我会带你到一所偏僻的农村小学看看。2018年，我找到的中国教育理念最先进的小学不是北京或上海名校，而是山区里的一所农村小学。你不必吃惊，社会发展的剧情经常会有令人意想不到的转变。]]></content>
      <categories>
        <category>Book</category>
      </categories>
      <tags>
        <tag>Book</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[High-level Semantic Feature Detection A New Perspective for Pedestrian Detection]]></title>
    <url>%2F2019%2F05%2F29%2FCVPR2019-CSP%2F</url>
    <content type="text"><![CDATA[Paper Link Code 简介CSP: Center and Scale Prediction 方法实验 代码准备ground truth12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def calc_gt_center(C, img_data,r=2, down=4,scale=&apos;h&apos;,offset=True): def gaussian(kernel): sigma = ((kernel-1) * 0.5 - 1) * 0.3 + 0.8 s = 2*(sigma**2) dx = np.exp(-np.square(np.arange(kernel) - int(kernel / 2)) / s) return np.reshape(dx,(-1,1)) gts = np.copy(img_data[&apos;bboxes&apos;]) igs = np.copy(img_data[&apos;ignoreareas&apos;]) scale_map = np.zeros((int(C.size_train[0]/down), int(C.size_train[1]/down), 2)) if scale==&apos;hw&apos;: scale_map = np.zeros((int(C.size_train[0] / down), int(C.size_train[1] / down), 3)) if offset: offset_map = np.zeros((int(C.size_train[0] / down), int(C.size_train[1] / down), 3)) seman_map = np.zeros((int(C.size_train[0]/down), int(C.size_train[1]/down), 3)) seman_map[:,:,1] = 1 if len(igs) &gt; 0: igs = igs/down for ind in range(len(igs)): x1,y1,x2,y2 = int(igs[ind,0]), int(igs[ind,1]), int(np.ceil(igs[ind,2])), int(np.ceil(igs[ind,3])) seman_map[y1:y2, x1:x2,1] = 0 # 被忽视的区域在第１个通道上置０ if len(gts)&gt;0: gts = gts/down for ind in range(len(gts)): # x1, y1, x2, y2 = int(round(gts[ind, 0])), int(round(gts[ind, 1])), int(round(gts[ind, 2])), int(round(gts[ind, 3])) x1, y1, x2, y2 = int(np.ceil(gts[ind, 0])), int(np.ceil(gts[ind, 1])), int(gts[ind, 2]), int(gts[ind, 3]) c_x, c_y = int((gts[ind, 0] + gts[ind, 2]) / 2), int((gts[ind, 1] + gts[ind, 3]) / 2) dx = gaussian(x2-x1) dy = gaussian(y2-y1) gau_map = np.multiply(dy, np.transpose(dx)) seman_map[y1:y2, x1:x2,0] = np.maximum(seman_map[y1:y2, x1:x2,0], gau_map) # 在第０个通道上置高斯值 seman_map[y1:y2, x1:x2,1] = 1 # 前景在第１个通道上置１ seman_map[c_y, c_x, 2] = 1 # 在第２个通道上目标中心位置１ if scale == &apos;h&apos;: scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 0] = np.log(gts[ind, 3] - gts[ind, 1]) scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 1] = 1 elif scale==&apos;w&apos;: scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 0] = np.log(gts[ind, 2] - gts[ind, 0]) scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 1] = 1 elif scale==&apos;hw&apos;: scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 0] = np.log(gts[ind, 3] - gts[ind, 1]) scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 1] = np.log(gts[ind, 2] - gts[ind, 0]) scale_map[c_y-r:c_y+r+1, c_x-r:c_x+r+1, 2] = 1 if offset: offset_map[c_y, c_x, 0] = (gts[ind, 1] + gts[ind, 3]) / 2 - c_y - 0.5 offset_map[c_y, c_x, 1] = (gts[ind, 0] + gts[ind, 2]) / 2 - c_x - 0.5 offset_map[c_y, c_x, 2] = 1 if offset: return seman_map,scale_map,offset_map else: return seman_map, scale_map seman_map有三个位面，第一个是高斯值mask，第二个是学习权重，第三个是目标中心点的位置。 网络结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def nn_p3p4p5(img_input=None, offset=True, num_scale=1, trainable=False): bn_axis = 3 x = ZeroPadding2D((3, 3))(img_input) x = Convolution2D(64, (7, 7), strides=(2, 2), name=&apos;conv1&apos;, trainable=False)(x) x = BatchNormalization(axis=bn_axis, name=&apos;bn_conv1&apos;)(x) x = Activation(&apos;relu&apos;)(x) x = MaxPooling2D((3, 3), strides=(2, 2), padding=&apos;same&apos;)(x) x = conv_block(x, 3, [64, 64, 256], stage=2, block=&apos;a&apos;, strides=(1, 1), trainable=False) x = identity_block(x, 3, [64, 64, 256], stage=2, block=&apos;b&apos;, trainable=False) stage2 = identity_block(x, 3, [64, 64, 256], stage=2, block=&apos;c&apos;, trainable=False) # print(&apos;stage2: &apos;, stage2._keras_shape[1:]) x = conv_block(stage2, 3, [128, 128, 512], stage=3, block=&apos;a&apos;, trainable=trainable) x = identity_block(x, 3, [128, 128, 512], stage=3, block=&apos;b&apos;, trainable=trainable) x = identity_block(x, 3, [128, 128, 512], stage=3, block=&apos;c&apos;, trainable=trainable) stage3 = identity_block(x, 3, [128, 128, 512], stage=3, block=&apos;d&apos;, trainable=trainable) # print(&apos;stage3: &apos;, stage3._keras_shape[1:]) x = conv_block(stage3, 3, [256, 256, 1024], stage=4, block=&apos;a&apos;, trainable=trainable) x = identity_block(x, 3, [256, 256, 1024], stage=4, block=&apos;b&apos;, trainable=trainable) x = identity_block(x, 3, [256, 256, 1024], stage=4, block=&apos;c&apos;, trainable=trainable) x = identity_block(x, 3, [256, 256, 1024], stage=4, block=&apos;d&apos;, trainable=trainable) x = identity_block(x, 3, [256, 256, 1024], stage=4, block=&apos;e&apos;, trainable=trainable) stage4 = identity_block(x, 3, [256, 256, 1024], stage=4, block=&apos;f&apos;, trainable=trainable) # print(&apos;stage4: &apos;, stage4._keras_shape[1:]) x = conv_block(stage4, 3, [512, 512, 2048], stage=5, block=&apos;a&apos;, strides=(1, 1), dila=(2, 2), trainable=trainable) x = identity_block(x, 3, [512, 512, 2048], stage=5, block=&apos;b&apos;, dila=(2, 2), trainable=trainable) stage5 = identity_block(x, 3, [512, 512, 2048], stage=5, block=&apos;c&apos;, dila=(2, 2), trainable=trainable) # print(&apos;stage5: &apos;, stage5._keras_shape[1:]) P3_up = Deconvolution2D(256, kernel_size=4, strides=2, padding=&apos;same&apos;, kernel_initializer=&apos;glorot_normal&apos;, name=&apos;P3up&apos;, trainable=trainable)(stage3) # print(&apos;P3_up: &apos;, P3_up._keras_shape[1:]) P4_up = Deconvolution2D(256, kernel_size=4, strides=4, padding=&apos;same&apos;, kernel_initializer=&apos;glorot_normal&apos;, name=&apos;P4up&apos;, trainable=trainable)(stage4) # print(&apos;P4_up: &apos;, P4_up._keras_shape[1:]) P5_up = Deconvolution2D(256, kernel_size=4, strides=4, padding=&apos;same&apos;, kernel_initializer=&apos;glorot_normal&apos;, name=&apos;P5up&apos;, trainable=trainable)(stage5) # print(&apos;P5_up: &apos;, P5_up._keras_shape[1:]) P3_up = L2Normalization(gamma_init=10, name=&apos;P3norm&apos;)(P3_up) P4_up = L2Normalization(gamma_init=10, name=&apos;P4norm&apos;)(P4_up) P5_up = L2Normalization(gamma_init=10, name=&apos;P5norm&apos;)(P5_up) conc = Concatenate(axis=-1)([P3_up, P4_up, P5_up]) feat = Convolution2D(256, (3, 3), padding=&apos;same&apos;, kernel_initializer=&apos;glorot_normal&apos;, name=&apos;feat&apos;, trainable=trainable)(conc) feat = BatchNormalization(axis=bn_axis, name=&apos;bn_feat&apos;)(feat) feat = Activation(&apos;relu&apos;)(feat) x_class = Convolution2D(1, (1, 1), activation=&apos;sigmoid&apos;, kernel_initializer=&apos;glorot_normal&apos;, bias_initializer=prior_probability_onecls(probability=0.01), name=&apos;center_cls&apos;, trainable=trainable)(feat) x_regr = Convolution2D(num_scale, (1, 1), activation=&apos;linear&apos;, kernel_initializer=&apos;glorot_normal&apos;, name=&apos;height_regr&apos;, trainable=trainable)(feat) if offset: x_offset = Convolution2D(2, (1, 1), activation=&apos;linear&apos;, kernel_initializer=&apos;glorot_normal&apos;, name=&apos;offset_regr&apos;, trainable=trainable)(feat) return [x_class, x_regr, x_offset] else: return [x_class, x_regr] Loss12345678910111213141516def cls_center(y_true, y_pred): classification_loss = K.binary_crossentropy(y_pred[:, :, :, 0], y_true[:, :, :, 2]) # firstly we compute the focal weight positives = y_true[:, :, :, 2] negatives = y_true[:, :, :, 1]-y_true[:, :, :, 2] foreground_weight = positives * (1.0 - y_pred[:, :, :, 0]) ** 2.0 background_weight = negatives * ((1.0 - y_true[:, :, :, 0])**4.0)*(y_pred[:, :, :, 0] ** 2.0) focal_weight = foreground_weight + background_weight assigned_boxes = tf.reduce_sum(y_true[:, :, :, 2]) class_loss = 0.01*tf.reduce_sum(focal_weight*classification_loss) / tf.maximum(1.0, assigned_boxes) assigned_boxes) return class_loss]]></content>
      <categories>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep High-Resolution Representation Learning for Human Pose Estimation]]></title>
    <url>%2F2019%2F05%2F23%2FCVPR19-HRNet%2F</url>
    <content type="text"><![CDATA[Paper Link Code 介绍High-Resolution Net (HRNet) 这篇论文解决人体姿态估计问题，重点关注学习高分辨率表示。大多数现有的方法通过一个由高到低分辨率的网络，从一个低分辨率的表示中恢复高分辨率的表示。相反，本论文提出的网络自始至终都保持了高分辨率的表示。 作者从高分辨率子网作为第一个阶段，逐渐添加 高-&gt;低 分辨率的子网形成更多的阶段，并且并行连接这些多分辨率子网。作者多次进行多尺度融合，使得每一个高-&gt;低分辨率的表示可以从其他并行的表示中接收信息，从而生成丰富的高分辨率表示。因此，预测的关键点热图可以更准确，空间更精确。作者在COCO关键点检测数据集和MPII Human Pose数据集上进行了验证。 与其他方法之前的区别 该方法并行级联高-&gt;低分辨率子网，而不是以序列的方式。因此，该方法保持了高分辨率，而不是从低分辨率中恢复高分辨率。所以预测的热图的空间上更准确。 现存的融合策略集成底层（low-level）和高层（high-level）的表示。 而该方法通过相似深度和相同层级的低分辨率表示的帮助，执行重复的多尺度融合提升高分率表示。 图1展示了提出的HRNet网络的结构。它包含并行的高-&gt;低分辨率的子网，重复的在不同分辨率子网之间的信息交换，即多尺度融合。水平和垂直方向分别对应于网络的深度和特征图的比例。 图2展示了其他方法的一些网络结构，这些方法都是依靠high-to-low和low-to-high框架的姿态估计网络结构。其中（a)表示Hourglass网络，（b）表示Cascaded pyramid networks，（c）表示SimpleBaseline网络：转置卷积（transposed convolutions）用于low-to-high过程。（d）组合空洞卷积（dilated convolutions）。 方法介绍序列多尺度子网用$N_{sr}$表示子网络在第s个stage，r表示分辨率的序号，它的分辨率是第一个子网络分辨率的$\frac{1}{r^{r-1}}$倍。有S=4个stege的high-to-low网络可以表示为： $$N_{11} \to N_{22} \to N_{33} \to N_{44}$$ 并行多尺度子网我们从一个高分辨率子网络作为第一个stage起始，逐渐地增加high-to-low分辨率子网络，形成新的sgates，并且并行地连接多分辨率子网络。因此，后一阶段并行子网的分辨率包括前一阶段的分辨率和一个更低的分辨率。 这里给出一个网络结构的例子，包含4个并行的子网络，如下： 重复多尺度融合 图3展示了交换单元（Exchange Unit）如何为高、中和底层融合信息的。右侧的注释表示：strided 3x3=stride 3x3卷积，up samp. 1x1=最近邻上采样和一个1x1卷积。 我们在不同的并行子网之间引入交换单元（exchange unit），这样每个子网可以重叠地从其他并行网络中接收信息。这里给出了一个交换信息框架的例子，如下图表示的结构。我们将第三个stage分成几个exchange blocks，并且每一个block有三个并行的卷积单元构成，一个交换单元在并行的卷积单元之间，如下： 其中，$C^b_{sr}$表示在第s个stage，第b个block的第r分辨率的卷积单元。$\varepsilon^b_s$是对应的交换的单元。 交换单元如图3所示。 热图估计我们简单地从最后一个交换单元（exhcange unit）输出的高分辨率表示中回归热图。损失函数（定义为均方误差）用于比较预测的热图和groundtruth热图。通过应用2D高斯生成的groundtruth热图，其中标准偏差为1像素，并以每个关键点的标注位置为中心。 网络实例实验中提出了两种网络，一个小网络HRNet-W32，一个大网络HRNet-W48，其中32和48分别表示在后3个sgate中的高分辨率子网络的宽度（C）。对于HRNet-W32，其他三个并行的子网络的宽度分别是64，128,256，对于HRNet-W48是96，192，384。 实验 代码Exchange Unit1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def _make_fuse_layers(self): if self.num_branches == 1: return None num_branches = self.num_branches num_inchannels = self.num_inchannels fuse_layers = [] for i in range(num_branches if self.multi_scale_output else 1): fuse_layer = [] for j in range(num_branches): if j &gt; i: fuse_layer.append( nn.Sequential( nn.Conv2d( num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False ), nn.BatchNorm2d(num_inchannels[i]), nn.Upsample(scale_factor=2**(j-i), mode=&apos;nearest&apos;) ) ) elif j == i: fuse_layer.append(None) else: conv3x3s = [] for k in range(i-j): if k == i - j - 1: num_outchannels_conv3x3 = num_inchannels[i] conv3x3s.append( nn.Sequential( nn.Conv2d( num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False ), nn.BatchNorm2d(num_outchannels_conv3x3) ) ) else: num_outchannels_conv3x3 = num_inchannels[j] conv3x3s.append( nn.Sequential( nn.Conv2d( num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False ), nn.BatchNorm2d(num_outchannels_conv3x3), nn.ReLU(True) ) ) fuse_layer.append(nn.Sequential(*conv3x3s)) fuse_layers.append(nn.ModuleList(fuse_layer)) return nn.ModuleList(fuse_layers) HighResolutionModule123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165class HighResolutionModule(nn.Module): def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels, fuse_method, multi_scale_output=True): super(HighResolutionModule, self).__init__() self._check_branches( num_branches, blocks, num_blocks, num_inchannels, num_channels) self.num_inchannels = num_inchannels self.fuse_method = fuse_method self.num_branches = num_branches self.multi_scale_output = multi_scale_output self.branches = self._make_branches( num_branches, blocks, num_blocks, num_channels) self.fuse_layers = self._make_fuse_layers() self.relu = nn.ReLU(True) def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels): if num_branches != len(num_blocks): error_msg = &apos;NUM_BRANCHES(&#123;&#125;) &lt;&gt; NUM_BLOCKS(&#123;&#125;)&apos;.format( num_branches, len(num_blocks)) logger.error(error_msg) raise ValueError(error_msg) if num_branches != len(num_channels): error_msg = &apos;NUM_BRANCHES(&#123;&#125;) &lt;&gt; NUM_CHANNELS(&#123;&#125;)&apos;.format( num_branches, len(num_channels)) logger.error(error_msg) raise ValueError(error_msg) if num_branches != len(num_inchannels): error_msg = &apos;NUM_BRANCHES(&#123;&#125;) &lt;&gt; NUM_INCHANNELS(&#123;&#125;)&apos;.format( num_branches, len(num_inchannels)) logger.error(error_msg) raise ValueError(error_msg) def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1): downsample = None if stride != 1 or \ self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion: downsample = nn.Sequential( nn.Conv2d( self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion, kernel_size=1, stride=stride, bias=False ), nn.BatchNorm2d( num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM ), ) layers = [] layers.append( block( self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample ) ) self.num_inchannels[branch_index] = \ num_channels[branch_index] * block.expansion for i in range(1, num_blocks[branch_index]): layers.append( block( self.num_inchannels[branch_index], num_channels[branch_index] ) ) return nn.Sequential(*layers) def _make_branches(self, num_branches, block, num_blocks, num_channels): branches = [] for i in range(num_branches): branches.append( self._make_one_branch(i, block, num_blocks, num_channels) ) return nn.ModuleList(branches) def _make_fuse_layers(self): if self.num_branches == 1: return None num_branches = self.num_branches num_inchannels = self.num_inchannels fuse_layers = [] for i in range(num_branches if self.multi_scale_output else 1): fuse_layer = [] for j in range(num_branches): if j &gt; i: fuse_layer.append( nn.Sequential( nn.Conv2d( num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False ), nn.BatchNorm2d(num_inchannels[i]), nn.Upsample(scale_factor=2**(j-i), mode=&apos;nearest&apos;) ) ) elif j == i: fuse_layer.append(None) else: conv3x3s = [] for k in range(i-j): if k == i - j - 1: num_outchannels_conv3x3 = num_inchannels[i] conv3x3s.append( nn.Sequential( nn.Conv2d( num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False ), nn.BatchNorm2d(num_outchannels_conv3x3) ) ) else: num_outchannels_conv3x3 = num_inchannels[j] conv3x3s.append( nn.Sequential( nn.Conv2d( num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False ), nn.BatchNorm2d(num_outchannels_conv3x3), nn.ReLU(True) ) ) fuse_layer.append(nn.Sequential(*conv3x3s)) fuse_layers.append(nn.ModuleList(fuse_layer)) return nn.ModuleList(fuse_layers) def get_num_inchannels(self): return self.num_inchannels def forward(self, x): if self.num_branches == 1: return [self.branches[0](x[0])] for i in range(self.num_branches): x[i] = self.branches[i](x[i]) x_fuse = [] for i in range(len(self.fuse_layers)): y = x[0] if i == 0 else self.fuse_layers[i][0](x[0]) for j in range(1, self.num_branches): if i == j: y = y + x[j] else: y = y + self.fuse_layers[i][j](x[j]) x_fuse.append(self.relu(y)) return x_fuse]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Pose Estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adaptive NMS Refining Pedestrian Detection in a Crowd]]></title>
    <url>%2F2019%2F05%2F20%2FAdaptive-NMS%2F</url>
    <content type="text"><![CDATA[paper link 简介这篇论文主要提出一个新颖的非极大值抑制（Non-Maximum Suppression, NMS）算法更好地改善检测器给出的检测框。本文主要贡献： 提出adaptive-NMS，该算法根据目标的密度使用一个动态抑制阈值。 设计一个高效网络学习密度得分，这个得分可以方便地嵌入到single-stage和two-stage检测器中。 实现了CityPersons和CrowdHuman数据集上的 state of the art 结果。 Motivation 图1展示了不同阈值下的greedy-NMS的结果。蓝色的框表示丢失的目标，红色的框表示假正例（false positives）。（b）中的检测框是Faster R-CNN在NMS之前的检测结果。如图c，一个低的NMS阈值可能会移除正例（true positives）。如同d，一个高的NMS阈值可能会增加假正例（false positives）。 在本文中，作者提出了一种新的NMS算法，名为adaptive-NMS，它可以作为人群中行人检测的更有效的替代方案。直观地，高NMS阈值保持更多拥挤的实例，而低NMS阈值消除更多误报。因此，自适应NMS应用动态抑制策略，其中阈值随着实例聚集和相互遮挡而上升，并且当实例单独出现时衰减。为此，我们设计了一个辅助且可学习的子网络来预测每个实例的自适应NMS阈值。 Adaptive-NMS 当物体处于拥挤区域时，增加NMS的阈值可以保留高覆盖率。同样，在稀疏场景下，应该去掉重复度高的候选框，因为它们很可能是假正例。 $$d_i:= \max_{b_j \in \mathcal{G}, i \neq j} \mathrm{iou}(b_i, b_j)$$ 目标$i$的密度被定义和在ground truth集合$\mathcal{G}$中的其他目标的最大紧致框的IoU的值。目标的密度表示拥挤遮挡的程度。 使用这个定义，我们提出更新下面策略中的移除步骤， $$N_\mathcal{M} := \max(N_t, d_\mathcal{M})$$ $N_t$表示对于$\mathcal{M}$的adaptive NMS的阈值，$d_{\mathcal{M}}$表示目标$\mathcal{M}$覆盖的密度。 这个抑制策略有三个性质： 当相邻的框远离$\mathcal{M}$时，即$\mathrm{iou}(\mathcal{M}, b_i) &lt; N_t$，它们与原始NMS保持一致。 如果$\mathcal{M}$定位到拥挤的区域，即$d_{\mathcal{M}} &gt; N_t$，$\mathcal{M}$的密度被使用作为adaptive NMS的阈值。 对与稀疏区域的目标，即$d_{\mathcal{M}} \leq N_t$，NMS阈值$N_\mathcal{M}$和原始NMS阈值相等，非常接近的框被作为假正例所抑制。 这个算法具体步骤如图2所示。 Density Prediction 作者把密度估计作为一个回归问题，目标密度值的计算根据它的定义，使用Smooth-L1损失函数作为训练损失。 一个天然的方式就是为这个回归在网络顶部添加一个并行的层，像分类和定位一样。然而，用于检测的特征仅仅包含目标自己的信息，比如外表、语义特征和位置。对于密度估计，使用独立目标的信息很难估计其密度，密度估计需要使用其周围目标的更多的线索。 为了解决这个，作者设计了一个额外的网络，它由三层卷积层构成，如图3所示。首选使用一个1x1的卷积层做特征维度降维，然后级联降维后的特征、用于RPN分类的特征和用于RPN回归的特征。最后使用一个大尺度的卷积核5x5作为最后的卷积层，为了把周围的信息送入网络。具体如图中Density subnet绿色框区域结构。 Experiments]]></content>
      <categories>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Squeeze-and-Excitation Networks]]></title>
    <url>%2F2019%2F05%2F17%2FSqueeze-and-Excitation-Networks%2F</url>
    <content type="text"><![CDATA[SENet介绍卷积神经网络（CNNs）的核心模块是卷积操作，这个操作使得网络能够通过每层的局部感受野融合空间和通道的信息，来构建有信息的特征。之前大量的工作已经研究了这种关系的空间组成部分，试图通过提高整个特征层次中空间编码的质量来增强CNN的表征能力。在这项工作中，作者将重点放在通道关系上，并且提出一个新的构架单元，成为“Squeeze-and-Excitation”（SE）块，通过明确地建模通道之间的相互依赖性来自适应地重新校准通道方面的特征响应。 Squeeze-and-Excitation Blocks Squeeze: 全局信息嵌入 Excition: 适应性地校准 实例化到ResNet和Inception 代码CaffeCaffe SENet 第三方实现 Caffe. SE-mudolues are integrated with a modificated ResNet-50 using a stride 2 in the 3x3 convolution instead of the first 1x1 convolution which obtains better performance: Repository. TensorFlow. SE-modules are integrated with a pre-activation ResNet-50 which follows the setup in fb.resnet.torch: Repository. TensorFlow. Simple Tensorflow implementation of SENets using Cifar10: Repository. MatConvNet. All the released SENets are imported into MatConvNet: Repository. MXNet. SE-modules are integrated with the ResNeXt and more architectures are coming soon: Repository. PyTorch. Implementation of SENets by PyTorch: Repository. Chainer. Implementation of SENets by Chainer: Repository. Pytorch实现SE模块来自https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py的se_module.py文件 1234567891011121314151617181920from torch import nnclass SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y.expand_as(x)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pillow Tutorial]]></title>
    <url>%2F2019%2F04%2F11%2FPillow-Tutorial%2F</url>
    <content type="text"><![CDATA[Pillow 教程PIL(Python Image Library)是python的第三方图像处理库。其官方主页为:PIL。 PIL历史悠久，原来是只支持python2.x的版本的，后来出现了移植到python3的库pillow, pillow号称是friendly fork for PIL。 Pillow is the friendly PIL fork by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors. for Python2Python Imaging Library (PIL) for Python3The Python Imaging Library Handbook python-pillow Pillow Github Pillow Docs 123from PIL import Imageimport matplotlib.pyplot as plt%matplotlib inline 从一个文件加载图片，使用在Image模块下的open()函数。 1im = Image.open("lena.jpg") 1. 使用Image类如果成功，这个函数会返回一个Image目标。你可以使用实例属性去测试这个文件的内容： 12from __future__ import print_functionprint(im.format, im.size, im.mode) JPEG (512, 512) RGB format属性判断图片的来源。如果一个图片不是从一个文件读入，它会被设置成None。size属性是一个2元组，包含宽和高。mode属性定义图片通道的数量和名字，以及像素类型和深度。通常的模式包含：“L”（luminance）表示灰度图像，”RGB“表示真彩色图像，”CMYK“表示印前（pre-press）图像 如果文件没有被打开，会给出IOError。 一旦你有一个Image类的实例，你可以使用这个类定义的方法取处理这个图片。例如，显示我们刚加载的图片： 1im.show() 使用matplotlib进行可视化 1plt.imshow(im) &lt;matplotlib.image.AxesImage at 0x7f1173b85518&gt; 2. 读取和写入图像3. 剪切、复制和合并图像Image类包含允许您操作图像中的区域的方法。要从图像中提取矩形区域，请使用crop()方法。 从一个图片中复制一个矩形区域123box = (100, 100, 400, 400)region = im.crop(box)print(region.size) (300, 300) 这个区域使用一个4元组定义，它的坐标是(left, upper, right, lower)。Python Imaging Library在左上角使用(0, 0)的坐标系统。坐标表示像素之间的位置，因此上边例子中的区域是300x300像素。 处理一个矩形区域，并且把它粘贴回原来位置123region = region.transpose(Image.ROTATE_180)im.paste(region, box)plt.imshow(im) &lt;matplotlib.image.AxesImage at 0x7f11721e2518&gt; 当把区域粘贴回去，区域的尺寸必须相同。除此之外，这个区域不能超越图像的边界。 12345678910111213def roll(image, delta): """Roll an image sideways.""" xsize, ysize = image.size delta = delta % xsize if delta == 0: return image part1 = image.crop((0, 0, delta, ysize)) part2 = image.crop((delta, 0, xsize, ysize)) image.paste(part1, (xsize-delta, 0, xsize, ysize)) image.paste(part2, (0, 0, xsize-delta, ysize)) return image 拆分和合并通道123r, g, b = im.split()im = Image.merge("RGB", (b, g, r))plt.imshow(im) &lt;matplotlib.image.AxesImage at 0x7f11721c9b38&gt; 保存图像1im.save(r'out.jpg') 新建图像12newIm= Image.new('RGB', (50, 50), 'red')plt.imshow(newIm) &lt;matplotlib.image.AxesImage at 0x7f1175c2d630&gt; 123# 十六进制颜色newIm = Image.new('RGBA',(100, 50), '#FF0000')plt.imshow(newIm) &lt;matplotlib.image.AxesImage at 0x7f1175bfd8d0&gt; 1234# 传入元组形式的RGBA值或者RGB值# 在RGB模式下，第四个参数失效，默认255，在RGBA模式下，也可只传入前三个值，A值默认255newIm = Image.new('RGB',(200, 100), (255, 255, 0, 120))plt.imshow(newIm) &lt;matplotlib.image.AxesImage at 0x7f1175bd1be0&gt; 复制图片12copyIm = im.copy()copyIm.size (512, 512) 调整图片大小123width, height = copyIm.sizeresizedIm = im.resize((width, int(0.5* height)))resizedIm.size (512, 256) 4. 几何变换1234im = Image.open("lena.jpg")out = im.resize((128, 128))out = im.rotate(45) # degrees counter-clockwiseplt.imshow(out) &lt;matplotlib.image.AxesImage at 0x7f11721370b8&gt; 转置图像123456out = im.transpose(Image.FLIP_LEFT_RIGHT)out = im.transpose(Image.FLIP_TOP_BOTTOM)out = im.transpose(Image.ROTATE_90)out = im.transpose(Image.ROTATE_180)out = im.transpose(Image.ROTATE_270)plt.imshow(out) &lt;matplotlib.image.AxesImage at 0x7f1172111080&gt; 5. 颜色变换在不同models之间转换123from PIL import Imageim = Image.open("lena.jpg").convert("L")print(im.mode) L 6. 图片增强滤波器123from PIL import ImageFilterout = im.filter(ImageFilter.DETAIL)plt.imshow(out) &lt;matplotlib.image.AxesImage at 0x7f1172073860&gt; 123# 高斯模糊out = im.filter(ImageFilter.GaussianBlur)plt.imshow(out) &lt;matplotlib.image.AxesImage at 0x7f117204f470&gt; 123# 边缘增强im.filter(ImageFilter.EDGE_ENHANCE)plt.imshow(out) &lt;matplotlib.image.AxesImage at 0x7f1171faf0b8&gt; 1234567891011121314# 普通模糊im.filter(ImageFilter.BLUR)# 找到边缘im.filter(ImageFilter.FIND_EDGES)# 浮雕im.filter(ImageFilter.EMBOSS)# 轮廓im.filter(ImageFilter.CONTOUR)# 锐化im.filter(ImageFilter.SHARPEN)# 平滑im.filter(ImageFilter.SMOOTH)# 细节im.filter(ImageFilter.DETAIL) 应用点变换12# multiply each pixel by 1.2out = im.point(lambda i: i * 1.2) 增强图像12345from PIL import ImageEnhanceenh = ImageEnhance.Contrast(im)out = enh.enhance(1.3)plt.imshow(out) &lt;matplotlib.image.AxesImage at 0x7f1171f906a0&gt; 7. 图片序列1234567891011from PIL import Imageim = Image.open("chi.gif")im.seek(1) # skip to the second frametry: while 1: im.seek(im.tell()+1) # do something to imexcept EOFError: pass # end of sequence 1plt.imshow(im) &lt;matplotlib.image.AxesImage at 0x7f1171eec208&gt; 使用ImageSequence Iterator类12345678from PIL import ImageSequencecount = 1for frame in ImageSequence.Iterator(im): # ...do something to frame... count += 1 if count % 10 == 0: plt.imshow(frame) plt.show() 8. Postscript printingDrawing Postscript12345678910111213141516171819from PIL import Imagefrom PIL import PSDrawim = Image.open("hopper.ppm")title = "hopper"box = (1*72, 2*72, 7*72, 10*72) # in pointsps = PSDraw.PSDraw() # default is sys.stdoutps.begin_document(title)# draw the image (75 dpi)ps.image(box, im, 75)ps.rectangle(box)# draw titleps.setfont("HelveticaNarrow-Bold", 36)ps.text((3*72, 4*72), title)ps.end_document() %!PS-Adobe-3.0 save /showpage { } def %%EndComments %%BeginDocument /S { show } bind def /P { moveto show } bind def /M { moveto } bind def /X { 0 rmoveto } bind def /Y { 0 exch rmoveto } bind def /E { findfont dup maxlength dict begin { 1 index /FID ne { def } { pop pop } ifelse } forall /Encoding exch def dup /FontName exch def currentdict end definefont pop } bind def /F { findfont exch scalefont dup setfont [ exch /setfont cvx ] cvx bind def } bind def /Vm { moveto } bind def /Va { newpath arcn stroke } bind def /Vl { moveto lineto stroke } bind def /Vc { newpath 0 360 arc closepath } bind def /Vr { exch dup 0 rlineto exch dup neg 0 exch rlineto exch neg 0 rlineto 0 exch rlineto 100 div setgray fill 0 setgray } bind def /Tm matrix def /Ve { Tm currentmatrix pop translate scale newpath 0 0 .5 0 360 arc closepath Tm setmatrix } bind def /Vf { currentgray exch setgray fill setgray } bind def %%EndProlog gsave 226.560000 370.560000 translate 0.960000 0.960000 scale gsave 10 dict begin /buf 384 string def 128 128 scale 128 128 8 [128 0 0 -128 0 128] { currentfile buf readhexstring pop } bind false 3 colorimage %%%%EndBinary grestore end grestore 72 144 M 504 720 0 Vr /PSDraw-HelveticaNarrow-Bold ISOLatin1Encoding /HelveticaNarrow-Bold E /F0 36 /PSDraw-HelveticaNarrow-Bold F 216 288 M (hopper) S %%EndDocument restore showpage %%End 9. 更多关于图像读取123from PIL import Imageim = Image.open("hopper.ppm")plt.imshow(im) &lt;matplotlib.image.AxesImage at 0x7f1171d5a400&gt; 1234from PIL import Imagewith open("hopper.ppm", "rb") as fp: im = Image.open(fp) plt.imshow(im) 10. 控制解码器使用草稿（draft）模式读取12345im = Image.open("lena.jpg")print("original =", im.mode, im.size)im.draft("L", (100, 100))print("draft =", im.mode, im.size) original = RGB (512, 512) draft = L (128, 128) 12]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Panoptic Feature Pyramid Networks]]></title>
    <url>%2F2019%2F01%2F11%2FPanoptic-Feature-Pyramid-Networks%2F</url>
    <content type="text"><![CDATA[论文链接：http://arxiv.org/abs/1901.02446 Abstract全景分割(panoptic segmentation) 实例分割(instance segmentation)(for thing classes) 语义分割(semantic segmentation)(for stuff classes) 之前的一些方法使用了分开的不相似的网络做实例分割和语义分割，它们之间没有任何共享计算。在这个工作中，作者在结构层面统一了这些方法，设计了一个单一网络用于这些任务。 该方法使用一个语义分割分支扩展Mask-RCNN，使其具有全景分割的能力。该分支使用了一个共享的特征金字塔网络主干。该方法不仅保持了实例分割的性能，也得到了一个轻量级权重的、高性能的语义分割方法。 图２展示了Panoptic Feature Pyramid Networks的预测结果。 Panoptic Feature Pyramid Network该网络结构在原有的Feature Pyramid Network的基础上，保持了原有的Instance segmentation branch，新提出了Panoptic FPN。 Panoptic FPN: As discussed, our approach is to modify Mask R-CNN with FPN to enable pixel-wise semantic seg- mentation prediction. However, to achieve accurate predic- tions, the features used for this task should: (1) be of suit- ably high resolution to capture fine structures, (2) encode sufficiently rich semantics to accurately predict class labels, and (3) capture multi-scale information to predict stuff re- gions at multiple resolutions. Although FPN was designed for object detection, these requirements – high-resolution, rich, multi-scale features – identify exactly the characteris- tics of FPN. We thus propose to attach to FPN a simple and fast semantic segmentation branch, described next. Semantic segmentation branch 图３显示了语义分割分支。左侧的每个FPN层通过卷积和双线性插值上采样被上采样，直到它达到1/4的尺度（右侧），这些输出被加和，并且最终转换为一个像素级别的输出。 重点介绍语义分割分支： 为了从FPN特征生成语义分割的输出，作者提出一个简单的设计去融合来自FPN金字塔层中所有信息到一个单一的输出，如上图３所示。开始与最深层的FPN(在尺度1/32上)，我们执行山歌上采样阶段去产生一个1/4尺度的特征图，这里每一个上采样阶段包含3x3卷积，group norm, ReLU，和2x bilinear upsampling。这些策略被重复用在其他FPN尺度上(1/16,1/8和1/4)，并且渐进地减少上采样步骤。这个结果是在1/4尺度的输出特征层的一个集合，该集合是通过元素相加的方法组合成的。1x1卷积，4x bilinear upsampling和softmax被用于在原始图像分辨率上生成每个像素的类别标签。 图５是用于增加分辨率的骨干网结构。(a)是标准的卷积网络，(维度定义为#blocksx#channels#xresolution)。(b)通过使用空洞卷积(dilated convolutions)来减少卷积的步长。(c)一个U-Net风格的网络，使用一个对称的解码器镜像bottom-up通路，但是是反向的。(d)FPN可以被视为一个非对称的、轻权重的解码器，它的top-down通路中每个stage仅仅有一个block，并且使用了一个相同的通道维度。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Focal Loss for Dense Object Detection]]></title>
    <url>%2F2019%2F01%2F10%2FFocal-Loss%2F</url>
    <content type="text"><![CDATA[本文代码：https://github.com/facebookresearch/Detectron 本文主要解决在one-stage的密集检测器中，大量前景和背景不平衡的问题。作者通过降低被很好分类样本的权重来解决类别不平衡的问题。Focal Loss集中于在稀疏难样本(hard examples)上的训练，并且在训练中防止大量的容易的反例(easy negatives)淹没检测器。 提出Focal Loss, 解决正负样本不平衡问题; 提出one-stage检测模型，RetinaNet。 作者提出一个新的损失函数 Focal Loss, 该方法通过添加因子$(1-p_t)^\gamma$到标准的交叉熵损失函数。设定$\gamma&gt;0$会减少被很好分类样本($p&gt;0.5$)的相对损失，更加关注于难和错误分类的样本。 图中显示了，RetinaNet检测器使用了focal loss，结果由于之前的one-stage和two-stage检测器。 类别不平衡问题在R-CNN这一类检测器中，类别不平衡问题通过two-stage级联和采样策略被解决。在候选区域提取阶段，Selective Search, EdgeBoxes, RPN等方法，缩小候选区域位置的数量到１~2k个，大量过滤掉了背景。在第二分类阶段，采样策略，例如固定前景背景比率(1:3)，或者在线难样本挖掘(OHEM)方法被执行用于保持前景和背景的一个可控的平衡。 Focal LossFocal Loss被设计用以解决one-state目标检测器训练中大量正反例样本不平衡的问题，通常是(1:1000)。我们首先介绍二值分类的交叉熵损失: $$ CE(p, y)=\begin{cases}-log(p) &amp; y=1\-log(1-p) &amp; otherwise.\end{cases} $$ 3.1 Balanced Cross Entropy一种通用的解决类别不平衡的方法是对类别1引入一个权重因子$\alpha \in [0, 1]$，对类别class-1引入$1-\alpha$。我们写成$\alpha-$balanced CE loss: $$CE(p_t)=-\alpha_t log(p_t)$$ 3.2 Focal Loss Definition试验中显示，在训练dense detectors是遭遇的大量类别不平衡会压倒交叉熵损失。容易分类的样本会占损失的大部分，并且主导梯度。尽管$\alpha$平衡了正负(positive/negative)样本的重要性，但是它没有区分易和难的样本(easy/hard)。相反，作者提出的更改过后的损失函数降低了容易样本的权重并且集中训练难反例样本。 我们定义focal loss： $$FL(p_t)=-(1-p_t)^\gamma log(p_t)$$ 3.4 Class Imbalance and Two-stage DetectorsTwo-stage检测器通常没有使用$\alpha-$balancing 或者我们提出的loss。代替这些，他们使用了两个机制来解决类别不平衡问题：(1) 一个两级的级联，(2) 有偏置的小批量采样。第一级联阶段的候选区域提取机制减少了大量可能的候选位置。重要的是，这些选择的候选框不是随机的，而是选择更像前景的可能位置，这样就移除了大量的容易的难反例样本(easy negatives)。第二阶段，有偏置的采样通常使用1:3比率的正负样本构建小批量(minibatches)。这个采样率类似$\alpha-$balancing因子，并且通过采样来实现。作者提出的focal loss主要设计用于解决one-stage检测系统中的这些问题。]]></content>
      <categories>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 安装与使用]]></title>
    <url>%2F2018%2F10%2F10%2Fdocker%2F</url>
    <content type="text"><![CDATA[Docker 安装与使用@(工具学习记录)[Docker] 1. Docker安装参考官网教程 卸载旧的版本1$ sudo apt-get remove docker docker-engine docker.iSET UP THE REPOSITORY SET UP THE REPOSITORY 1$ sudo apt-get update 12345$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common 1$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 123456$ sudo apt-key fingerprint 0EBFCD88pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) &lt;docker@docker.com&gt;sub 4096R/F273FCD8 2017-02-22 x86_64/amd641234$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 安装 DOCKER CE更新包的索引1$ sudo apt-get update 安装最新版本的Docker CE1$ sudo apt-get install docker-ce 安装特定版本Docker CE123$ apt-cache madison docker-cedocker-ce | 18.03.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages 验证是否安装正确1$ sudo docker run hello-world 2.nvidia-docker 安装参考nvidia-dockernstalling version 2.0 Debian-based distributions123456curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \ sudo apt-key add -distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \ sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get update 12sudo apt-get install nvidia-docker2sudo pkill -SIGHUP dockerd 3. detectron 配置参考detectron install12cd $DETECTRON/dockerdocker build -t detectron:c2-cuda9-cudnn7 . 运行这个镜像1nvidia-docker run --rm -it detectron:c2-cuda9-cudnn7 python detectron/tests/test_batch_permutation_op.py 4. docker 基本命令对容器生命周期管理rundocker run ：创建一个新的容器并运行一个命令12345678910111213141516使用docker镜像nginx:latest以后台模式启动一个容器,并将容器命名为mynginx。docker run --name mynginx -d nginx:latest使用镜像nginx:latest以后台模式启动一个容器,并将容器的80端口映射到主机随机端口。docker run -P -d nginx:latest使用镜像 nginx:latest，以后台模式启动一个容器,将容器的 80 端口映射到主机的 80 端口,主机的目录 /data 映射到容器的 /data。docker run -p 80:80 -v /data:/data -d nginx:latest绑定容器的 8080 端口，并将其映射到本地主机 127.0.0.1 的 80 端口上。$ docker run -p 127.0.0.1:80:8080/tcp ubuntu bash使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。runoob@runoob:~$ docker run -it nginx:latest /bin/bashroot@b8573233d675:/# start/stop/restartkillrmdocker rm ：删除一个或多少容器 语法1docker rm [OPTIONS] CONTAINER [CONTAINER...] OPTIONS说明： -f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷docker rm ：删除一个或多少容器 语法docker rm [OPTIONS] CONTAINER [CONTAINER…]OPTIONS说明： -f :通过SIGKILL信号强制删除一个运行中的容器 -l :移除容器间的网络连接，而非容器本身 -v :-v 删除与容器关联的卷123456789强制删除容器db01、db02docker rm -f db01 db02移除容器nginx01对容器db01的连接，连接名dbdocker rm -l db 删除容器nginx01,并删除容器挂载的数据卷docker rm -v nginx01 pause/unpausecreateexecdocker exec ：在运行的容器中执行命令12345678在容器mynginx中以交互模式执行容器内/root/runoob.sh脚本runoob@runoob:~$ docker exec -it mynginx /bin/sh /root/runoob.shhttp://www.runoob.com/在容器mynginx中开启一个交互模式的终端runoob@runoob:~$ docker exec -i -t mynginx /bin/bashroot@b1a0703e41e7:/# commit 命令docker commit :从容器创建一个新的镜像。 -a :提交的镜像作者； -c :使用Dockerfile指令来创建镜像； -m :提交时的说明文字； -p :在commit时，将容器暂停。 将容器a404c6c174a2 保存为新的镜像,并添加提交人信息和说明信息。12345runoob@runoob:~$ docker commit -a &quot;runoob.com&quot; -m &quot;my apache&quot; a404c6c174a2 mymysql:v1 sha256:37af1236adef1544e8886be23010b66577647a40bc02c0885a6600b33ee28057runoob@runoob:~$ docker images mymysql:v1REPOSITORY TAG IMAGE ID CREATED SIZEmymysql v1 37af1236adef 15 seconds ago 329 MB 容器与本地之间拷贝文件12345将主机./RS-MapReduce目录拷贝到容器30026605dcfe的/home/cloudera目录下。docker cp RS-MapReduce 30026605dcfe:/home/cloudera将容器30026605dcfe的/home/cloudera/RS-MapReduce目录拷贝到主机的/tmp目录中。docker cp 30026605dcfe:/home/cloudera/RS-MapReduce /tmp/ 5. 学习资源 runoob docker 只要一小时，零基础入门Docker]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conda使用指南]]></title>
    <url>%2F2018%2F09%2F09%2FConda-Tutorials%2F</url>
    <content type="text"><![CDATA[Python渐渐成为最流行的编程语言之一，在数据分析、机器学习和深度学习等方向Python语言更是主流。Python的版本比较多，并且它的库也非常广泛，同时库和库之间存在很多依赖关系，所以在库的安装和版本的管理上很麻烦。Conda是一个管理版本和Python环境的工具，它使用起来非常容易。 首先你需要安装Anconda软件，点击链接download。选择对应的系统和版本类型。 Conda的环境管理创建环境12# 创建一个名为python34的环境，指定Python版本是3.5（不用管是3.5.x，conda会为我们自动寻找3.５.x中的最新版本）conda create --name py35 python=3.5 激活环境123456789# 安装好后，使用activate激活某个环境activate py35 # for Windowssource activate py35 # for Linux &amp; Mac(py35) user@user-XPS-8920:~$ # 激活后，会发现terminal输入的地方多了py35的字样，实际上，此时系统做的事情就是把默认2.7环境从PATH中去除，再把3.4对应的命令加入PATH (py35) user@user-XPS-8920:~$ python --versionPython 3.5.5 :: Anaconda, Inc.# 可以得到`Python 3.5.5 :: Anaconda, Inc.`，即系统已经切换到了3.５的环境 返回主环境123# 如果想返回默认的python 2.7环境，运行deactivate py35 # for Windowssource deactivate py35 # for Linux &amp; Mac 删除环境12# 删除一个已有的环境conda remove --name py35 --all 查看系统中的所有环境用户安装的不同Python环境会放在~/anaconda/envs目录下。查看当前系统中已经安装了哪些环境，使用conda info -e。 1234567user@user-XPS-8920:~$ conda info -e# conda environments:#base * /home/user/anaconda2caffe /home/user/anaconda2/envs/caffepy35 /home/user/anaconda2/envs/py35tf /home/user/anaconda2/envs/tf Conda的包管理安装库为当前环境安装库123# numpyconda install numpy# conda会从从远程搜索numpy的相关信息和依赖项目 ### 查看已经安装的库 123# 查看已经安装的packagesconda list# 最新版的conda是从site-packages文件夹中搜索已经安装的包，可以显示出通过各种方式安装的包 查看某个环境的已安装包12# 查看某个指定环境的已安装包conda list -n py35 搜索package的信息12# 查找package信息conda search numpy 12345678Loading channels: done# Name Version Build Channel numpy 1.5.1 py26_1 pkgs/free ...numpy 1.15.1 py37hec00662_0 anaconda/pkgs/main numpy 1.15.1 py37hec00662_0 pkgs/main 安装package到指定的环境1234# 安装packageconda install -n py35 numpy# 如果不用-n指定环境名称，则被安装在当前活跃环境# 也可以通过-c指定通过某个channel安装 更新package12# 更新packageconda update -n py35 numpy 删除package12# 删除packageconda remove -n py35 numpy 更新conda12# 更新conda，保持conda最新conda update conda 更新anaconda12# 更新anacondaconda update anaconda 更新Python123# 更新pythonconda update python# 假设当前环境是python 3.5, conda会将python升级为3.5.x系列的当前最新版本 设置国内镜像因为Anaconda.org的服务器在国外，所有有些库下载缓慢，可以使用清华Anaconda镜像源。 网站地址: 清华大学开源软件镜像站 Anaconda 镜像Anaconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 下载。 TUNA还提供了Anaconda仓库的镜像，运行以下命令：123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 即可添加 Anaconda Python 免费仓库。 运行 conda install numpy 测试一下吧。 Miniconda 镜像Miniconda 是一个 Anaconda 的轻量级替代，默认只包含了 python 和 conda，但是可以通过 pip 和 conda 来安装所需要的包。 Miniconda 安装包可以到 https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ 下载。]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[MLIA] Logistic Regression]]></title>
    <url>%2F2018%2F09%2F05%2FMLIA-Logistic-Regression%2F</url>
    <content type="text"><![CDATA[Logistic Regression本代码来自Machine Learning in Action。 想要了解更多的朋友可以参考此书。 Sigmoid函数$$\sigma(z) = \frac{1}{(1+e^{-z})}$$ 12345678910import numpy as npimport matplotlib.pyplot as pltdef sigmoid(inX): return 1.0/(1+np.exp(-inX))z = np.linspace(-5, 5, 100)y = sigmoid(z)plt.plot(z, y)plt.show() 1234z = np.linspace(-60, 60, 100)y = sigmoid(z)plt.plot(z, y)plt.show() Sigmoid函数类似一个单位阶跃函数。当x＝0时，Sigmoid函数值为0.5；随着x增大，Sigmoid函数值将逼近于1；随着x减小，Sigmoid函数将逼近于0。利用这个性质可以对它的输入做一个二分类。 为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把它的所有的结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间是数值。当大于0.5的时候，将数据分类为1；当小于0.5的时候，将数据分类为0。 Sigmoid函数的输入记为z: $$z=w_0x_0 + w_1x_1 + w_2x_2 + \cdot \cdot \cdot + w_n x_n$$ Sigmoid函数的导数Sigmoid导数具体推导过程如下： $$\begin{align}f^{\prime}(z) &amp;= (\frac{1}{1+e^{-z}})^{\prime}\\&amp;=\frac{e^{-z}}{(1+e^{-z})^2}\\&amp;=\frac{1+e^{-z}-1}{(1+e^{-z})^2}\\&amp;=\frac{1}{(1+e^{-z})}(1-\frac{1}{(1+e^{-z})})\\&amp;=f(z)(1-f(z))\end{align}$$ 梯度上升法梯度上升法：顾名思义就是利用梯度方向，寻找到某函数的最大值。 梯度上升算法迭代公式：$$w:=w+\alpha \nabla_w f(w)$$ 梯度下降法：和梯度上升想法，利用梯度方法，寻找某个函数的最小值。梯度下降算法迭代公式：$$w:=w-\alpha \nabla_w f(w)$$ 梯度上升算法每次更新之后，都会重新估计移动的方法，即梯度。 Logistic 回归梯度上升优化法加载数据12345678def loadDataSet(): dataMat = []; labelMat = [] fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat,labelMat 12345dataArray, labelMat = loadDataSet()print("Total: ", len(dataArray))print("The first sample: ", dataArray[0])print("The second sample: ", dataArray[1])print("Label: ", labelMat) (&apos;Total: &apos;, 100) (&apos;The first sample: &apos;, [1.0, -0.017612, 14.053064]) (&apos;The second sample: &apos;, [1.0, -1.395634, 4.662541]) (&apos;Label: &apos;, [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0]) 数据集梯度上升123456789101112131415def sigmoid(inX): return 1.0/(1+np.exp(-inX))def gradAscent(dataMatIn, classLabels): dataMatrix = np.mat(dataMatIn) #convert to NumPy matrix labelMat = np.mat(classLabels).transpose() #convert to NumPy matrix m,n = np.shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = np.ones((n,1)) for k in range(maxCycles): #heavy on matrix operations h = sigmoid(dataMatrix*weights) #matrix mult error = (labelMat - h) #vector subtraction weights = weights + alpha * dataMatrix.transpose()* error #matrix mult return weights 1gradAscent(dataArray, labelMat) matrix([[ 4.12414349], [ 0.48007329], [-0.6168482 ]]) 绘制数据和决策边界123456789101112131415161718192021def plotBestFit(weights): import matplotlib.pyplot as plt dataMat,labelMat=loadDataSet() dataArr = np.array(dataMat) n = np.shape(dataArr)[0] xcord1 = []; ycord1 = [] xcord2 = []; ycord2 = [] for i in range(n): if int(labelMat[i])== 1: xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') x = np.arange(-3.0, 3.0, 0.1) y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x, y) plt.xlabel('X1'); plt.ylabel('X2'); plt.show() 12weights = gradAscent(dataArray, labelMat)plotBestFit(weights.getA()) １个epoch的随机梯度上升梯度上升算法在每次更新系数的时候都需要便利整个数据集，如果数据集的样本比较大，该方法的复杂度和计算代价就很高。有一种改进的方法叫做随机梯度上升方法。该方法的思想是选取一个样本，计算该样本的梯度，更新系数，再选取下一个样本。 123456789def stocGradAscent0(dataMatrix, classLabels): m,n = np.shape(dataMatrix) alpha = 0.01 weights = np.ones(n) #initialize to all ones for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights 12weights = stocGradAscent0(np.array(dataArray), labelMat)plotBestFit(weights) 上图之后遍历了一次数据集，这样的模型还处于欠拟合状态。需要多次遍历数据集才能优化好模型，接下来我们会运行200次迭代。 200个epoch的随机梯度上升1234567891011121314def stocGradAscent0(dataMatrix, classLabels): X0, X1, X2 = [], [], [] m,n = np.shape(dataMatrix) alpha = 0.01 weights = np.ones(n) #initialize to all ones for j in range(200): for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] X0.append(weights[0]) X1.append(weights[1]) X2.append(weights[2]) return weights, X0, X1, X2 12weights, X0, X1, X2 = stocGradAscent0(np.array(dataArray), labelMat)plotBestFit(weights) 可视化权重(weights)的变化12345fig, ax = plt.subplots(3, 1, figsize=(10, 5))ax[0].plot(np.arange(len(X0)), np.array(X0))ax[1].plot(np.arange(len(X1)), np.array(X1))ax[2].plot(np.arange(len(X2)), np.array(X2))plt.show() 从上图可以看出，算法正在逐渐收敛。由于数据集并不是线性可分的，所以存在一些不能正确分类的样本点，每次更新权重引起了周期的变化。 更新过后的随机梯度上升算法 学习率alpha会在每次迭代之后调整。 采用随机选取样本的更新策略，减少周期性的波动。 1234567891011121314151617def stocGradAscent1(dataMatrix, classLabels, numIter=150): X0, X1, X2 = [], [], [] m,n = np.shape(dataMatrix) weights = np.ones(n) #initialize to all ones for j in range(numIter): dataIndex = range(m) for i in range(m): alpha = 4/(1.0+j+i)+0.0001 #apha decreases with iteration, does not randIndex = int(np.random.uniform(0,len(dataIndex)))#go to 0 because of the constant h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] X0.append(weights[0]) X1.append(weights[1]) X2.append(weights[2]) del(dataIndex[randIndex]) return weights, X0, X1, X2 12weights, X0, X1, X2 = stocGradAscent1(np.array(dataArray), labelMat)plotBestFit(weights) 可视化权重(weights)的变化12345fig, ax = plt.subplots(3, 1, figsize=(10, 5))ax[0].plot(np.arange(len(X0)), np.array(X0))ax[1].plot(np.arange(len(X1)), np.array(X1))ax[2].plot(np.arange(len(X2)), np.array(X2))plt.show() 示例：从疝气病症预测病马的死亡率12345678910111213141516171819202122232425262728293031323334def classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): frTrain = open('horseColicTraining.txt', 'r'); frTest = open('horseColicTest.txt', 'r') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights, X0, X1, X2 = stocGradAscent1(np.array(trainingSet), trainingLabels, 1000) errorCount = 0; numTestVec = 0.0 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(np.array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print "the error rate of this test is: %f" % errorRate return errorRatedef multiTest(): numTests = 10; errorSum=0.0 for k in range(numTests): errorSum += colicTest() print "after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests)) 1multiTest() /home/tianliang/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp the error rate of this test is: 0.328358 the error rate of this test is: 0.432836 the error rate of this test is: 0.388060 the error rate of this test is: 0.373134 the error rate of this test is: 0.373134 the error rate of this test is: 0.447761 the error rate of this test is: 0.343284 the error rate of this test is: 0.313433 the error rate of this test is: 0.328358 the error rate of this test is: 0.462687 after 10 iterations the average error rate is: 0.379104]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CornerNet: Detection Objects as Paired Keypoints]]></title>
    <url>%2F2018%2F09%2F02%2FCornerNet-Detection-Objects-as-Paired-Keypoints%2F</url>
    <content type="text"><![CDATA[前言CornerNet: Detection Objects as Paired Keypoints 这篇论文发表在ECCV2018，本人感觉非常有意思，所以和大家分享一下。 Arxiv: https://arxiv.org/abs/1808.01244Github: https://github.com/umich-vl/ 介绍传统的目标检测都是给出紧致的候选框，本论文独具匠心，通过一对关键点（目标的左上角和右下角）来检测一个目标框。通过检测关键点的这种方式，可以消除利用先验知识设计anchor boxes这个需求。作者提出角点池化（corner pooling），角点池化可以帮助网络更好的定位角点。最终实验表明，CornerNet在MS COCO数据集上实现了42.1%的AP，优于所有现存的单级(one-stage)检测器。]]></content>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测（Pedestrian Detection）论文整理]]></title>
    <url>%2F2018%2F08%2F17%2FPedestrian-Detection-Sources%2F</url>
    <content type="text"><![CDATA[相关科研工作者 Piotr Dollár scholar Piotr Dollár homepage 张姗姗 scholar 张姗姗 homepage 欧阳万里 scholar 欧阳万里 homepage Paper List [CVPR-2019] High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection [CVPR-2019] SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection [CVPR-2019] Pedestrian Detection in Thermal Images using Saliency Maps [TIP-2018] Too Far to See? Not Really:- Pedestrian Detection with Scale-Aware Localization Policy [ECCV-2018] Bi-box Regression for Pedestrian Detection and Occlusion Estimation [ECCV-2018] Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting [ECCV-2018] Graininess-Aware Deep Feature Learning for Pedestrian Detection [ECCV-2018] Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd [ECCV-2018] Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation [CVPR-2018] Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors [CVPR-2018] Occluded Pedestrian Detection Through Guided Attention in CNNs [CVPR-2018] Repulsion Loss: Detecting Pedestrians in a Crowd [TCSVT-2018] Pushing the Limits of Deep CNNs for Pedestrian Detection [Trans Multimedia-2018] Scale-aware Fast R-CNN for Pedestrian Detection [TPAMI-2017] Jointly Learning Deep Features, Deformable Parts, Occlusion and Classification for Pedestrian Detection [BMVC-2017] PCN: Part and Context Information for Pedestrian Detection with CNNs [CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection [CVPR-2017] Learning Cross-Modal Deep Representations for Robust Pedestrian Detection [CVPR-2017] What Can Help Pedestrian Detection? [ICCV-2017] Multi-label Learning of Part Detectors for Heavily Occluded Pedestrian Detection [ICCV-2017] Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation [TPAMI-2017] Towards Reaching Human Performance in Pedestrian Detection [Transactions on Multimedia-2017] Scale-Aware Fast R-CNN for Pedestrian Detection [CVPR-2016] Semantic Channels for Fast Pedestrian Detection [CVPR-2016] How Far are We from Solving Pedestrian Detection? ![CVPR-2016] Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry ![CVPR-2016] Semantic Channels for Fast Pedestrian Detection ![ECCV-2016] Is Faster R-CNN Doing Well for Pedestrian Detection? [CVPR-2015] Taking a Deeper Look at Pedestrians ![ICCV-2015] Learning Complexity-Aware Cascades for Deep Pedestrian Detection [ICCV-2015] Deep Learning Strong Parts for Pedestrian Detection ![ECCV-2014] Deep Learning of Scene-specific Classifier for Pedestrian Detection [CVPR-2013] Joint Deep Learning for Pedestrian Detection [CVPR-2012] A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling [CVPR-2010] Multi-Cue Pedestrian Classification With Partial Occlusion Handling [CVPR-2009] Pedestrian detection: A benchmark [CVPR-2008] People-Tracking-by-Detection and People-Detection-by-Tracking [ECCV-2006] Human Detection Using Oriented Histograms of Flow and Appearance [CVPR-2005] Histograms of Oriented Gradients for Human Detection 论文[CVPR-2019] Adaptive NMS: Refining Pedestrian Detection in a Crowd paper: https://arxiv.org/abs/1904.02948 [CVPR-2019] High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection paper: https://arxiv.org/abs/1904.02948 github: https://github.com/liuwei16/CSP [CVPR-2019] SSA-CNN: Semantic Self-Attention CNN for Pedestrian Detection paper: https://arxiv.org/abs/1902.09080v1 [CVPR-2019] Pedestrian Detection in Thermal Images using Saliency Maps paper: https://arxiv.org/abs/1904.06859 [TIP-2018] Too Far to See? Not Really: Pedestrian Detection with Scale-Aware Localization Policy paper: project website: slides: github: [Transactions on Multimedia-201８] Scale-Aware Fast R-CNN for Pedestrian Detection paper: https://ieeexplore.ieee.org/abstract/document/8060595/ project website: slides: github: [ECCV-2018] Bi-box Regression for Pedestrian Detection and Occlusion Estimation arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf slides: github: [ECCV-2018] Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf project website: slides: github: [ECCV-2018] Graininess-Aware Deep Feature Learning for Pedestrian Detection arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/Chunze_Lin_Graininess-Aware_Deep_Feature_ECCV_2018_paper.pdf project website: slides: github: [ECCV-2018] Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd arxiv:http://arxiv.org/abs/1807.08407 project website: slides: github: [ECCV-2018] Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation arxiv:https://arxiv.org/abs/1807.01438 project website: slides: github: [CVPR-2018] Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors arxiv: paper: http://vision.snu.ac.kr/projects/partgridnet/data/noh_2018.pdf project website: http://vision.snu.ac.kr/projects/partgridnet/ slides: github: [CVPR-2018] Occluded Pedestrian Detection Through Guided Attention in CNNs arxiv: paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf project website: slides: github: [CVPR-2018] Repulsion Loss: Detecting Pedestrians in a Crowd arxiv:http://arxiv.org/abs/1711.07752 project website: slides: github: blog: https://zhuanlan.zhihu.com/p/41288115 [TPAMI-2017] Jointly Learning Deep Features, Deformable Parts, Occlusion and Classification for Pedestrian Detection paper: https://ieeexplore.ieee.org/abstract/document/8008790/ project website: slides: github caffe: [BMVC-2017] PCN: Part and Context Information for Pedestrian Detection with CNNs arxiv: https://arxiv.org/abs/1804.044838 project website: slides: github caffe: [CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection arxiv: http://arxiv.org/abs/1702.05693 project website: slides: github caffe: [CVPR-2017] Learning Cross-Modal Deep Representations for Robust Pedestrian Detection arxiv: https://arxiv.org/abs/1704.02431 project website: slides: github caffe: [CVPR-2017] What Can Help Pedestrian Detection? arxiv: https://arxiv.org/abs/1704.02431 project website: slides: github caffe: [TPAMI-2017] Towards Reaching Human Performance in Pedestrian Detection paper: http://ieeexplore.ieee.org/document/7917260/ arxiv: project website: slides: github caffe: [ICCV-2017] Multi-label Learning of Part Detectors for Heavily Occluded Pedestrian Detection paper: http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Multi-Label_Learning_of_ICCV_2017_paper.pdf arxiv: project website: slides: [ICCV-2017]Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation arxiv: https://arxiv.org/abs/1706.08564 project website: http://cvlab.cse.msu.edu/project-pedestrian-detection.html slides: github caffe: https://github.com/garrickbrazil/SDS-RCNN [CVPR-2016] Semantic Channels for Fast Pedestrian Detection paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Costea_Semantic_Channels_for_CVPR_2016_paper.pdf project website: slides: github caffe: [CVPR-2016] How Far areWe from Solving Pedestrian Detection? paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S06-29.pdf project website: slides: github caffe: [ICCV-2015] Deep Learning Strong Parts for Pedestrian Detection paper: https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tian_Deep_Learning_Strong_ICCV_2015_paper.htmler.html project website: slides: github caffe: [CVPR-2013] Joint Deep Learning for Pedestrian Detection Wanli paper: https://www.cv-foundation.org/openaccess/content_iccv_2013/html/Ouyang_Joint_Deep_Learning_2013_ICCV_paper.html project website: slides: github caffe: [CVPR-2012] A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling paper: http://mmlab.ie.cuhk.edu.hk/pdf/ouyangWcvpr2012.pdf paper: https://ieeexplore.ieee.org/abstract/document/6248062/ project website: slides: github caffe: [CVPR-2010] Multi-Cue Pedestrian Classification With Partial Occlusion Handling paper: https://ieeexplore.ieee.org/abstract/document/5540111/ project website: slides: github caffe: 行人检测数据集CityPersons CityPersons数据集是在Cityscapes数据集基础上建立的，使用了Cityscapes数据集的数据，对一些类别进行了精确的标注。该数据集是在[CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection这篇论文中提出的，更多细节可以通过阅读该论文了解。 上图中左侧是行人标注，右侧是原始的CityScapes数据集。 标注和评估文件 数据集下载 文件格式 123456789101112131415#评测文件$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/coco.py$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/eval_demo.py$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/eval_MR_multisetup.py#注释文件$/Cityscapes/shanshanzhang-citypersons/annotations$/Cityscapes/shanshanzhang-citypersons/annotations/anno_train.mat$/Cityscapes/shanshanzhang-citypersons/annotations/anno_val.mat$/Cityscapes/shanshanzhang-citypersons/annotations/README.txt#图片数据$/Cityscapes/leftImg8bit/train/*$/Cityscapes/leftImg8bit/val/*$/Cityscapes/leftImg8bit/test/* 注释文件格式123456789101112131415161718192021222324CityPersons annotations(1) data structure: one image per cell in each cell, there are three fields: city_name; im_name; bbs (bounding box annotations)(2) bounding box annotation format: one object instance per row: [class_label, x1,y1,w,h, instance_id, x1_vis, y1_vis, w_vis, h_vis](3) class label definition: class_label =0: ignore regions (fake humans, e.g. people on posters, reflections etc.) class_label =1: pedestrians class_label =2: riders class_label =3: sitting persons class_label =4: other persons with unusual postures class_label =5: group of people(4) boxes: visible boxes [x1_vis, y1_vis, w_vis, h_vis] are automatically generated from segmentation masks; (x1,y1) is the upper left corner. if class_label==1 or 2 [x1,y1,w,h] is a well-aligned bounding box to the full body ; else [x1,y1,w,h] = [x1_vis, y1_vis, w_vis, h_vis]; Caltech Caltech官网更所细节请阅读这篇论文，[TAPAMI-2012] Pedestrian Detection: An Evaluation of the State of the Art KITTI KITTI官网]]></content>
      <tags>
        <tag>Pedestrian Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras Tutorial]]></title>
    <url>%2F2018%2F08%2F14%2FKeras-Tutorial%2F</url>
    <content type="text"><![CDATA[Github地址：here Keras-Tutorials 版本：0.0.1 作者：张天亮 邮箱：zhangtianliang13@mails.ucas.ac.cn Github 加载 .ipynb 的速度较慢，建议在 Nbviewer 中查看该项目。 简介大部分内容来自keras项目中的examples 目录 01.多层感知机实现 02.模型的保存 03.模型的加载 04.绘制精度和损失曲线 05.卷积神经网络实现 06.CIFAR10_cnn 07.mnist_lstm 08.VGG16调用 09.卷积滤波器可视化 10.variational_autoencoder 11.锁定层fine-tuning网络 12.使用sklearn wrapper进行的参数搜索 13.Keras和Tensorflow联合使用 14.Finetune InceptionV3样例 15.自编码器 16.卷积自编码器 更多Keras使用方法请查看手册 中文手册 英文手册 github]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Nets]]></title>
    <url>%2F2018%2F08%2F14%2FGenerative-Adversarial-Nets%2F</url>
    <content type="text"><![CDATA[DCGANs in TensorFlowcarpedm20/DCGAN-tensorflow我们定义网络结构： 123456789101112131415161718192021222324252627282930313233343536def generator(self, z): self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, 'g_h0_lin', with_w=True) self.h0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = conv2d_transpose(h0, [self.batch_size, 8, 8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = conv2d_transpose(h1, [self.batch_size, 16, 16, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = conv2d_transpose(h2, [self.batch_size, 32, 32, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = conv2d_transpose(h3, [self.batch_size, 64, 64, 3], name='g_h4', with_w=True) return tf.nn.tanh(h4)def discriminator(self, image, reuse=False): if reuse: tf.get_variable_scope().reuse_variables() h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) h4 = linear(tf.reshape(h3, [-1, 8192]), 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 当我们初始化这个类时，我们将使用这些函数来创建模型。 我们需要两个版本的鉴别器共享（或重用）参数。 一个用于来自数据分布的图像的minibatch，另一个用于来自发生器的图像的minibatch。 123self.G = self.generator(self.z)self.D, self.D_logits = self.discriminator(self.images)self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) 接下来我们定义损失函数。我们在D的预测值和我们理想的判别器输出值之间使用交叉熵，而没有只用求和，因为这样的效果更好。判别器希望对“真实”数据的预测全部是1，并且来自生成器的“假”数据的预测全部是零。生成器希望判别器对所有假样本的预测都是1。 1234567891011self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))self.d_loss = self.d_loss_real + self.d_loss_fakeself.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))) 收集每个模型的变量，以便可以单独进行训练。 1234t_vars = tf.trainable_variables()self.d_vars = [var for var in t_vars if 'd_' in var.name]self.g_vars = [var for var in t_vars if 'g_' in var.name] 现在我们准备好优化参数，我们将使用ADAM，这是一种在现代深度学习中常见的自适应非凸优化方法。ADAM通常与SGD竞争，并且（通常）不需要手动调节学习速率，动量和其他超参数。 1234d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.d_loss, var_list=self.d_vars)g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.g_loss, var_list=self.g_vars) 我们已经准备好了解我们的数据。在每个epoch中，我们在每个minibatch中采样一些图像，并且运行优化器更新网络。有趣的是，如果G仅更新一次，判别器的损失则不会为零。另外，我认为d_loss_fake和d_loss_real在最后的额外的调用回到是一点点不必要的计算，并且是冗余的，因为这些值是作为d_optim和g_optim的一部分计算的。作为TensorFlow中的练习，您可以尝试优化此部分并将RP发送到原始库。 1234567891011121314151617181920212223for epoch in xrange(config.epoch): ... for idx in xrange(0, batch_idxs): batch_images = ... batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \ .astype(np.float32) # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict=&#123; self.images: batch_images, self.z: batch_z &#125;) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z &#125;) # Run g_optim twice to make sure that d_loss does not go to zero # (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z &#125;) errD_fake = self.d_loss_fake.eval(&#123;self.z: batch_z&#125;) errD_real = self.d_loss_real.eval(&#123;self.images: batch_images&#125;) errG = self.g_loss.eval(&#123;self.z: batch_z&#125;) Generative Adversarial Networks代码整理 InfoGAN-TensorFlow:InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets iGAN-Theano:Generative Visual Manipulation on the Natural Image Manifold SeqGAN-TensorFlow:SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient DCGAN-Tensorflow:Deep Convolutional Generative Adversarial Networks dcgan_code-Theano:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks improved-gan-Theano:Improved Techniques for Training GANs chainer-DCGAN:Chainer implementation of Deep Convolutional Generative Adversarial Network keras-dcgan]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to use hexo?]]></title>
    <url>%2F2018%2F08%2F14%2FHow-to-use-hexo%2F</url>
    <content type="text"><![CDATA[Hexo基本指令init1$ hexo init [folder] 新建一个网站，如果没有设置｀folder｀，Hexo默认在目前的文件夹建立网站。 new1$ hexo new [layout] &lt;title&gt; 新建一片文章。如果没有设置layout的话，默认使用_config.yml中的default_layout参数代替。如果标题包含空格的话，请使用引号括起来。 generate12$ hexo generate$ hexo g # 简写 publish1$ hexo publish [layout] &lt;filename&gt; 发表草稿。 server1$ hexo server 启动服务器。默认情况下，访问网址为：http://localhost:4000/。 选项 描述 -p,--port 重设端口 -s, --static 只使用静态文本 -l,--log 启动日记记录，使用覆盖记录格式 deploy12$ hexo deploy$ hexo d # 简写 render1$ hexo render &lt;file1&gt; [file2] ... 渲染文件。 clean1$ hexo clean 清楚缓存文件(db.json)好已经生成的静态文件(public)。 在某些情况(尤其是更换主题后)，如果发现您对站点的更改没有生效，可以使用此命令。 list1$ hexo list &lt;type&gt; 列出网站资料。 version1$ hexo version 显示Hexo版本。 显示草稿1$ hexo --draft 显示source/_drafts文件夹中的草稿文件。 自定义CWD1$ hexo --cwd /path/to/cwd 自定义当前工作目录(Current working dirctory)的路径。 在主页截断 方法1:在文中插入以下代码 1&lt;!--more--&gt; 方法2:在文章中的front-matter中添加description， 1234567---title: date: 2018-08-14 15:21:22categories: tags: description: 描述。。--- 分类和标签只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games 定义一段代码。12345import osimport numpy as npfor i in range(10): print('now is ', i) 显示一幅图像。1&#123;% asset_img 003.jpg This is an example image %&#125; Hexo 资源hexo官网 theme-next使用说明 使用hexo，如果换了电脑怎么更新博客？ 其他参考博客：我的个人博客之旅：从jekyll到hexoHEXO+NEXT主题个性化配置 hexo的next主题个性化配置教程 基于Hexo+Node.js+github+coding搭建个人博客——进阶篇(从入门到入土)]]></content>
  </entry>
</search>
