<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CornerNet: Detection Objects as Paired Keypoints]]></title>
    <url>%2F2018%2F09%2F02%2FCornerNet-Detection-Objects-as-Paired-Keypoints%2F</url>
    <content type="text"><![CDATA[前言CornerNet: Detection Objects as Paired Keypoints 这篇论文发表在ECCV2018，本人感觉非常有意思，所以和大家分享一下。 Arxiv: https://arxiv.org/abs/1808.01244Github: https://github.com/umich-vl/ 介绍传统的目标检测都是给出紧致的候选框，本论文独具匠心，通过一对关键点（目标的左上角和右下角）来检测一个目标框。通过检测关键点的这种方式，可以消除利用先验知识设计anchor boxes这个需求。作者提出角点池化（corner pooling），角点池化可以帮助网络更好的定位角点。最终实验表明，CornerNet在MS COCO数据集上实现了42.1%的AP，优于所有现存的单级(one-stage)检测器。]]></content>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测（Pedestrian Detection）论文整理]]></title>
    <url>%2F2018%2F08%2F17%2FPedestrian-Detection-Sources%2F</url>
    <content type="text"><![CDATA[相关科研工作者 张姗姗 scholar 张姗姗 homepage 论文[TIP-2018] Too Far to See? Not Really: Pedestrian Detection with Scale-Aware Localization Policy paper: project website: slides: github: [Transactions on Multimedia-201８] Scale-Aware Fast R-CNN for Pedestrian Detection paper: https://ieeexplore.ieee.org/abstract/document/8060595/ project website: slides: github: [ECCV-2018] Bi-box Regression for Pedestrian Detection and Occlusion Estimation arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/CHUNLUAN_ZHOU_Bi-box_Regression_for_ECCV_2018_paper.pdf slides: github: [ECCV-2018] Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.pdf project website: slides: github: [ECCV-2018] Graininess-Aware Deep Feature Learning for Pedestrian Detection arxiv: paper:http://openaccess.thecvf.com/content_ECCV_2018/papers/Chunze_Lin_Graininess-Aware_Deep_Feature_ECCV_2018_paper.pdf project website: slides: github: [ECCV-2018] Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd arxiv:http://arxiv.org/abs/1807.08407 project website: slides: github: [ECCV-2018] Small-scale Pedestrian Detection Based on Somatic Topology Localization and Temporal Feature Aggregation arxiv:https://arxiv.org/abs/1807.01438 project website: slides: github: [CVPR-2018] Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors arxiv: paper: http://vision.snu.ac.kr/projects/partgridnet/data/noh_2018.pdf project website: http://vision.snu.ac.kr/projects/partgridnet/ slides: github: [CVPR-2018] Occluded Pedestrian Detection Through Guided Attention in CNNs arxiv: paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf project website: slides: github: [CVPR-2018] Repulsion Loss: Detecting Pedestrians in a Crowd arxiv:http://arxiv.org/abs/1711.07752 project website: slides: github: blog: https://zhuanlan.zhihu.com/p/41288115 [BMVC-2017] PCN: Part and Context Information for Pedestrian Detection with CNNs arxiv: https://arxiv.org/abs/1804.044838 project website: slides: github: [ICCV-2017]Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation arxiv: https://arxiv.org/abs/1706.08564 project website: http://cvlab.cse.msu.edu/project-pedestrian-detection.html slides: github caffe: https://github.com/garrickbrazil/SDS-RCNN [CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection arxiv: http://arxiv.org/abs/1702.05693 project website: slides: github: [CVPR-2017] Learning Cross-Modal Deep Representations for Robust Pedestrian Detection arxiv: https://arxiv.org/abs/1704.02431 project website: slides: github: [CVPR-2017] What Can Help Pedestrian Detection? arxiv: https://arxiv.org/abs/1704.02431 project website: slides: github: [TPAMI-2017] Towards Reaching Human Performance in Pedestrian Detection paper: http://ieeexplore.ieee.org/document/7917260/ arxiv: project website: slides: github: [CVPR-2016] Semantic Channels for Fast Pedestrian Detection paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Costea_Semantic_Channels_for_CVPR_2016_paper.pdf project website: slides: github: [CVPR-2016] How Far areWe from Solving Pedestrian Detection? paper: https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S06-29.pdf project website: slides: github: 行人检测数据集CityPersons CityPersons数据集是在Cityscapes数据集基础上建立的，使用了Cityscapes数据集的数据，对一些类别进行了精确的标注。该数据集是在[CVPR-2017] CityPersons: A Diverse Dataset for Pedestrian Detection这篇论文中提出的，更多细节可以通过阅读该论文了解。 上图中左侧是行人标注，右侧是原始的CityScapes数据集。 标注和评估文件 数据集下载 文件格式 123456789101112131415#评测文件$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/coco.py$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/eval_demo.py$/Cityscapes/shanshanzhang-citypersons/evaluation/eval_script/eval_MR_multisetup.py#注释文件$/Cityscapes/shanshanzhang-citypersons/annotations$/Cityscapes/shanshanzhang-citypersons/annotations/anno_train.mat$/Cityscapes/shanshanzhang-citypersons/annotations/anno_val.mat$/Cityscapes/shanshanzhang-citypersons/annotations/README.txt#图片数据$/Cityscapes/leftImg8bit/train/*$/Cityscapes/leftImg8bit/val/*$/Cityscapes/leftImg8bit/test/* 注释文件格式123456789101112131415161718192021222324CityPersons annotations(1) data structure: one image per cell in each cell, there are three fields: city_name; im_name; bbs (bounding box annotations)(2) bounding box annotation format: one object instance per row: [class_label, x1,y1,w,h, instance_id, x1_vis, y1_vis, w_vis, h_vis](3) class label definition: class_label =0: ignore regions (fake humans, e.g. people on posters, reflections etc.) class_label =1: pedestrians class_label =2: riders class_label =3: sitting persons class_label =4: other persons with unusual postures class_label =5: group of people(4) boxes: visible boxes [x1_vis, y1_vis, w_vis, h_vis] are automatically generated from segmentation masks; (x1,y1) is the upper left corner. if class_label==1 or 2 [x1,y1,w,h] is a well-aligned bounding box to the full body ; else [x1,y1,w,h] = [x1_vis, y1_vis, w_vis, h_vis]; Caltech Caltech官网更所细节请阅读这篇论文，[TAPAMI-2012] Pedestrian Detection: An Evaluation of the State of the Art KITTI KITTI官网]]></content>
      <tags>
        <tag>Pedestrian Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras Tutorial]]></title>
    <url>%2F2018%2F08%2F14%2FKeras-Tutorial%2F</url>
    <content type="text"><![CDATA[Github地址：here Keras-Tutorials 版本：0.0.1 作者：张天亮 邮箱：zhangtianliang13@mails.ucas.ac.cn Github 加载 .ipynb 的速度较慢，建议在 Nbviewer 中查看该项目。 简介大部分内容来自keras项目中的examples 目录 01.多层感知机实现 02.模型的保存 03.模型的加载 04.绘制精度和损失曲线 05.卷积神经网络实现 06.CIFAR10_cnn 07.mnist_lstm 08.VGG16调用 09.卷积滤波器可视化 10.variational_autoencoder 11.锁定层fine-tuning网络 12.使用sklearn wrapper进行的参数搜索 13.Keras和Tensorflow联合使用 14.Finetune InceptionV3样例 15.自编码器 16.卷积自编码器 更多Keras使用方法请查看手册 中文手册 英文手册 github]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Nets]]></title>
    <url>%2F2018%2F08%2F14%2FGenerative-Adversarial-Nets%2F</url>
    <content type="text"><![CDATA[DCGANs in TensorFlowcarpedm20/DCGAN-tensorflow我们定义网络结构： 123456789101112131415161718192021222324252627282930313233343536def generator(self, z): self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4, 'g_h0_lin', with_w=True) self.h0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = conv2d_transpose(h0, [self.batch_size, 8, 8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = conv2d_transpose(h1, [self.batch_size, 16, 16, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = conv2d_transpose(h2, [self.batch_size, 32, 32, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = conv2d_transpose(h3, [self.batch_size, 64, 64, 3], name='g_h4', with_w=True) return tf.nn.tanh(h4)def discriminator(self, image, reuse=False): if reuse: tf.get_variable_scope().reuse_variables() h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) h4 = linear(tf.reshape(h3, [-1, 8192]), 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 当我们初始化这个类时，我们将使用这些函数来创建模型。 我们需要两个版本的鉴别器共享（或重用）参数。 一个用于来自数据分布的图像的minibatch，另一个用于来自发生器的图像的minibatch。 123self.G = self.generator(self.z)self.D, self.D_logits = self.discriminator(self.images)self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) 接下来我们定义损失函数。我们在D的预测值和我们理想的判别器输出值之间使用交叉熵，而没有只用求和，因为这样的效果更好。判别器希望对“真实”数据的预测全部是1，并且来自生成器的“假”数据的预测全部是零。生成器希望判别器对所有假样本的预测都是1。 1234567891011self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))self.d_loss = self.d_loss_real + self.d_loss_fakeself.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))) 收集每个模型的变量，以便可以单独进行训练。 1234t_vars = tf.trainable_variables()self.d_vars = [var for var in t_vars if 'd_' in var.name]self.g_vars = [var for var in t_vars if 'g_' in var.name] 现在我们准备好优化参数，我们将使用ADAM，这是一种在现代深度学习中常见的自适应非凸优化方法。ADAM通常与SGD竞争，并且（通常）不需要手动调节学习速率，动量和其他超参数。 1234d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.d_loss, var_list=self.d_vars)g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.g_loss, var_list=self.g_vars) 我们已经准备好了解我们的数据。在每个epoch中，我们在每个minibatch中采样一些图像，并且运行优化器更新网络。有趣的是，如果G仅更新一次，判别器的损失则不会为零。另外，我认为d_loss_fake和d_loss_real在最后的额外的调用回到是一点点不必要的计算，并且是冗余的，因为这些值是作为d_optim和g_optim的一部分计算的。作为TensorFlow中的练习，您可以尝试优化此部分并将RP发送到原始库。 1234567891011121314151617181920212223for epoch in xrange(config.epoch): ... for idx in xrange(0, batch_idxs): batch_images = ... batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \ .astype(np.float32) # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict=&#123; self.images: batch_images, self.z: batch_z &#125;) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z &#125;) # Run g_optim twice to make sure that d_loss does not go to zero # (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z &#125;) errD_fake = self.d_loss_fake.eval(&#123;self.z: batch_z&#125;) errD_real = self.d_loss_real.eval(&#123;self.images: batch_images&#125;) errG = self.g_loss.eval(&#123;self.z: batch_z&#125;) Generative Adversarial Networks代码整理 InfoGAN-TensorFlow:InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets iGAN-Theano:Generative Visual Manipulation on the Natural Image Manifold SeqGAN-TensorFlow:SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient DCGAN-Tensorflow:Deep Convolutional Generative Adversarial Networks dcgan_code-Theano:Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks improved-gan-Theano:Improved Techniques for Training GANs chainer-DCGAN:Chainer implementation of Deep Convolutional Generative Adversarial Network keras-dcgan]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to use hexo?]]></title>
    <url>%2F2018%2F08%2F14%2FHow-to-use-hexo%2F</url>
    <content type="text"><![CDATA[Hexo基本指令init1$ hexo init [folder] 新建一个网站，如果没有设置｀folder｀，Hexo默认在目前的文件夹建立网站。 new1$ hexo new [layout] &lt;title&gt; 新建一片文章。如果没有设置layout的话，默认使用_config.yml中的default_layout参数代替。如果标题包含空格的话，请使用引号括起来。 generate12$ hexo generate$ hexo g # 简写 publish1$ hexo publish [layout] &lt;filename&gt; 发表草稿。 server1$ hexo server 启动服务器。默认情况下，访问网址为：http://localhost:4000/。 选项 描述 -p,--port 重设端口 -s, --static 只使用静态文本 -l,--log 启动日记记录，使用覆盖记录格式 deploy12$ hexo deploy$ hexo d # 简写 render1$ hexo render &lt;file1&gt; [file2] ... 渲染文件。 clean1$ hexo clean 清楚缓存文件(db.json)好已经生成的静态文件(public)。 在某些情况(尤其是更换主题后)，如果发现您对站点的更改没有生效，可以使用此命令。 list1$ hexo list &lt;type&gt; 列出网站资料。 version1$ hexo version 显示Hexo版本。 显示草稿1$ hexo --draft 显示source/_drafts文件夹中的草稿文件。 自定义CWD1$ hexo --cwd /path/to/cwd 自定义当前工作目录(Current working dirctory)的路径。 在主页截断 方法1:在文中插入以下代码 1&lt;!--more--&gt; 方法2:在文章中的front-matter中添加description， 1234567---title: date: 2018-08-14 15:21:22categories: tags: description: 描述。。--- 分类和标签只有文章支持分类和标签，您可以在 Front-matter 中设置。在其他系统中，分类和标签听起来很接近，但是在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games 定义一段代码。12345import osimport numpy as npfor i in range(10): print('now is ', i) 显示一幅图像。1&#123;% asset_img 003.jpg This is an example image %&#125; Hexo 资源hexo官网 theme-next使用说明 使用hexo，如果换了电脑怎么更新博客？ 其他参考博客：我的个人博客之旅：从jekyll到hexoHEXO+NEXT主题个性化配置 hexo的next主题个性化配置教程 基于Hexo+Node.js+github+coding搭建个人博客——进阶篇(从入门到入土)]]></content>
  </entry>
</search>
